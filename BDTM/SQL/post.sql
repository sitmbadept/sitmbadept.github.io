DROP TABLE IF EXISTS posts;

CREATE TABLE posts(
	id int,
    post_title MEDIUMTEXT,
    post_text LONGTEXT
);



INSERT INTO `posts` (`id`, `post_title`, `post_text`) VALUES ('1', 'When Does a Trip End?', 'When Does a Trip End?\r\nA Yukon River Adventure in Alaska that will stay part of me forever.\r\nA yellow and two red canoes empty on the banks of the Yukon river.\r\nPhoto by author, Audrey Stimson.\r\nI often ask myself, “when does a trip end?”\r\n\r\nIs it when you stop hearing the twirling water splash against the fiberglass canoe? Or when you no longer see the eroded river banks where spindly trees lean over the water like limp children’s fishing poles ready to tumble into the rushing currents? Or maybe when you stop feeling the pinch of muscle aches just under your shoulder after hours of paddling down the river? Or is it when the horizon begins to vanish at the end of your perception of a memory of a mighty river so far away you barely believe it was real?\r\n\r\nI have not yet let go of my journey down Alaska’s Yukon though it’s been a week since we pulled our candy-colored canoes out of the river at a town called Circle. Circle, Alaska was not much of a place. It was a spot that punctuates the end of the road with a question mark.\r\n\r\nI didn’t yet want to break through the membrane which keeps me there. I didn’t want to enter into my default life of built things — airplanes, plastic-wrapped food, and asphalt paths we call freeways that wrap around me like constricting snakes.\r\n\r\nI wanted to hold onto my river adventure as if it were a dream I hadn’t finished dreaming.\r\n\r\nThe yellow moon rising over the Yukon river.\r\nPhoto by the author — Audrey Stimson.\r\nTravel always does that to me, especially if I let it enter me. It’s easy for me to do that in the wilderness. 8 days of disconnecting from everything helped get me far away.\r\n\r\nIn the last decade of my life as a traveler I have sought out places that let me explore my own states of being. I do that by looking for the small moments that contain everything that sustains me whether it be in a ray of sun over water, a quaking aspen on the crusty banks, a backlit bluff jetting out the water’s flow, a caw of a raven on a bending willow, the riffle of water as a salmon swims up to spawn, the spongy soft marsh, the sucking mud on a sand bar island, and the crackling of a campfire.\r\n\r\nThe canoe trip with a group of German fans of my friend Dirk Rohrbach, an adventurer and river guide, delivered those moments of enchantment in spades.\r\n\r\nFrom the moment I looked down on the snaking Tanana River and the vast tundra of muskeg marshes surrounded by the stunted forests of Alaska from my airplane window, I was hooked again.\r\n\r\nAlaska The Last Frontier always delivers the extraordinary in its vastness.\r\n\r\nThe 49th State is more than a place, it’s a state of mind.\r\n\r\nAlaska is far away, it is wild, it is abundant in space and a place. It is somewhere where you are smaller than anywhere else. The lower 48 states seem tiny in comparison.\r\n\r\nI like that feeling of being small. The smallness of me frees me.\r\n\r\nWhat does free mean?\r\n\r\nIt means there are no questions pressing against the wall of my insecurities about who or what I should be. I am, and that’s enough. I am just me in the wildness and accept it as if I have always been part of it.\r\n\r\nAlaska takes me to my primordial essence like a story that I heard but can’t remember who told it to me.\r\n\r\nAudrey — author smiling in canoe with Gregg her husband with a paddle in the back of canoe on the mud colored Yukon.\r\nPhoto by the author, Audrey Stimson.\r\nFloating 156 miles down the mighty Yukon River, with the slush of the paddle and the hiss from the glacial sand echoing off the side of the fiberglass canoe, I felt as if she guided me with a gentleness I have never experienced. The hard, cold, crisp, brittle space of Alaska filled me with a softness. Mother Nature cradled me in her arms and carried me through a magical quiet place, a place with no answers because there were no questions.\r\n\r\nThe dime-sized water spider skirted along the mud-colored flow, her webs laced like delicate strings illuminated by the sun ten feet above the water. Was it the spider’s escape? Or was the web a lifeline? Or maybe just a means to an end?\r\n\r\nBoiling water is what they call the spinning eddies moved by hidden obstacles deep below the surface of the river. Each twirl like a thought bubble or an unseen being letting us know everything will be alright if we let the river guide us.\r\n\r\nA rippling rush of water in front of us was a subtle hint of shallow water and the presence of rainbow-colored river rocks that were carried a thousand miles from where they began their life in the mineral-rich pinnacles of glacial mountain ranges.\r\n\r\nSunset at the Yukon with orange reflections. Gravel rocky banks of the Yukon.\r\nPhoto by the author, Audrey Stimson.\r\nA steaming caldera of a volcano in the distance off to our right reminded us that the earth is alive, growing and contracting, always changing.\r\n\r\nThe exposed roots of the spruce, the alder, and the aspen, were shallow against the blocks of white permafrost visible where the river ice break-up tore chunks from the melting land. It was a sign that climate change takes no prisoners even on the edge of the Arctic.\r\n\r\nThe moose bull relaxing on the moss-covered banks stares quizzically at the flotilla of humans just as surprised to see us as we were to see him.\r\n\r\nThe brown backs of the mother grizzly hunched over and busy while showing her cub the fine art of berry hunting. The brown bears were not 100 yards from where we rested our arms and ate our lunch in the M&M-colored boats.\r\n\r\nA flock of geese honked their disapproval. The birds were bothered by our movement as they feasted on the last fresh Alaskan vegetation just before beginning their journey south.\r\n\r\nIt all makes sense to me, a picture I will carry with me always as I slowly let myself come home and release the hold of a magical wilderness that simply doesn’t want to let go of me.\r\n\r\nMaybe tomorrow I will wake up inside my ordinary life. I will step back into my world of bills to pay, laundry to do, and work to perform.\r\n\r\nBut not just yet. Today I will let the mighty Yukon take me with her once more wherever she wants to go.\r\n\r\nIf you enjoy these stories and you also want to discover millions of other writers on Medium please consider subscribing for as little as $5 a month with the link below. Audrey will get a little slice of the pie if you do. Thank you for your support.\r\n\r\nJoin Medium with my referral link - Audrey Stimson\r\nRead every story from Audrey Stimson (and thousands of other writers on Medium). Your membership fee directly supports…\r\nmedium.com\r\n\r\nAudrey Stimson is a writer living in a green house with her husband and two dogs. When she is not writing essays and short stories she works as a television news producer. She is currently working on a forthcoming book about a bicycle trip across the United States. More about Audrey on her website .\r\n\r\nThanks to Bikegrog\r\n'), ('2', 'Can machines learn how to behave?', 'Beyond the current news cycle about whether AIs are sentient is a more practical and immediately consequential conversation about AI value alignment: whether and how AIs can be imbued with human values. Today, this turns on the even more fundamental question of whether the newest generation of language models can or can’t understand concepts — and on what it means to understand.¹\r\n\r\nIf, as some researchers contend, language models are mere “babblers” that randomly regurgitate their training data — “garbage in, garbage out” — then real AI value alignment is, at least for now, out of reach. Seemingly, the best we can do is to carefully curate training inputs to filter out “garbage”, often referred to as “toxic content”, even as we seek to broaden data sources to better represent human diversity. There are some profound challenges implied here, including governance (who gets to define what is “toxic”?), labor (is it humane to employ people to do “toxic content” filtering?²), and scale (how can we realistically build large models under such constraints?). This skeptical view also suggests a dubious payoff for the whole language model research program, since the practical value of a mere “babbler” is unclear: what meaningful tasks could a model with no understanding of concepts be entrusted to do? If the answer is none, then why bother with them at all?\r\n\r\nOn the other hand, if, as I’ll argue here, language models are able to understand concepts, then they’ll have far greater utility — though with this utility, we must also consider a wider landscape of potential harms and risks. Urgent social and policy questions arise too. When so many of us (myself included) make our living doing information work, what will it mean for the labor market, our economic model, and even our sense of purpose when so many of today’s desk jobs can be automated?\r\n\r\nThis is no longer a remote, hypothetical prospect, but attention to it has waned as AI denialism has gained traction. Many AI ethicists have narrowed their focus to the subset of language model problems consistent with the assumption that they understand nothing: their failure to work for digitally underrepresented populations, promulgation of bias, generation of deepfakes, and output of words that might offend.\r\n\r\nThese are serious issues. However, today’s AI models are becoming far more generally capable than this narrow focus implies. AI can engineer drugs³ (or poisons⁴), design proteins,⁵ write code,⁶ solve puzzles,⁷ model people’s states of mind,⁸ control robots in human environments,⁹ and plan strategies.¹⁰ These things are hard to dismiss as mere babble; they’ll increasingly involve substantive interactions with people and real outcomes in the world, either for good or for ill. If AIs are highly capable but malicious, or just clueless about right and wrong, then some of the dangerous outcomes could even resemble those popularized by the very different community of philosophers and researchers who have written, both more sensationally and less groundedly, about AI existential risk.¹¹\r\n\r\nIt’s becoming increasingly clear that these two disconnected camps in the AI ethics debate are each seeing only part of the picture. Those who are deeply skeptical about what AI can do haven’t acknowledged either the risk or the potential of the emerging generation of general-purpose AI.\r\n\r\nOn the other hand, while those in the existential risk camp have been expansive in their articulation of potential harms and benefits, they consider “Artificial General Intelligence” (AGI) to be so distant, mysterious, and inscrutable that it’ll emerge spontaneously in an “intelligence explosion” decades from now;¹² AGI might then proceed, perhaps due to some Douglas Adams-ish programming oversight, to turn the entire universe into paperclips, or worse.¹³\r\n\r\nSuch doomsday scenarios may have seemed credible in 2014, but they’re far less so now that we’re starting to understand the landscape better. Language modeling has proven to be the key to making the leap from the specialized machine learning applications of the 2010s to the general-purpose AI technology of the 2020s. The result is hardly an alien entity with inscrutable goals. Anyone can chat with a language-enabled model, and it can respond in ways so familiar that concern has shifted overnight from worrying about AI’s alienness to worrying about our tendency to anthropomorphize it. It’s all too human-like!\r\n\r\nAlthough anthropomorphism does pose its own risks,¹⁴ this familiarity is good news, in that it may make human value alignment far more straightforward than the existential risk community has imagined. This is because, although our biology endows us with certain pre-linguistic moral sentiments (such as care for offspring and in-group altruism, both of which we share with many other species), language generalizes these sentiments into ethical values, whether widely held or aspirational. Hence oral and written language have mediated the fields of ethics, moral philosophy, law, and religion for thousands of years.\r\n\r\nFor an AI model to behave according to a given set of ethical values, it has to be able to understand what those values are just as we would — via language. By sharing language with AIs, we can share norms and values with them too. We have early evidence that this approach works, and as language-enabled models improve generally, so too will their ability to behave according to ethical principles. This is the main point I hope to convey in this essay.\r\n\r\nIn itself, the ability to endow an AI with values isn’t a panacea. It doesn’t guarantee perfect judgment — an unrealistic goal for either human or machine. Nor does it address governance questions: who gets to define an AI’s values, and how much scope will these have for personal or cultural variation? Are some values better than others? How should AIs, their creators, and their users be held morally accountable? Neither does it tackle the economic problem articulated by John Maynard Keynes in 1930 — how to equitably distribute the collective gains of increasing automation,¹⁵ soon to include much intellectual labor.\r\n\r\nWhat it does offer is a clear route to imbuing an AI with values that are transparent, legible, and controllable by ordinary people. It also suggests mechanisms for addressing the narrower issues of bias and underrepresentation within the same framework.\r\n\r\nMy view is that AI values needn’t be — and shouldn’t be — dictated by engineers, ethicists, lawyers, or any other narrow constituency. Neither should they remain bulleted lists of desiderata posted on the web pages of standards bodies, governments, or corporations, with no direct connection to running code. They should, instead, become the legible and auditable “operating handbooks” of tomorrow’s AIs.\r\n\r\nMisunderstanding intelligence\r\nA proper history of AI is well beyond our scope here. However, a bit of historical context can help us trace a path from 20th century conceptions of AI, to the Deep Learning revolution of the 2010s, to the broad or general AI we’re starting to see emerge in the 2020s. This context helps fill the gap between some of the current debates about AI and today’s reality.\r\n\r\nGood Old Fashioned AI\r\nThe phrase “artificial intelligence” was coined by the organizers of the Dartmouth Summer Research Project on Artificial Intelligence in 1956. They held that “every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it”, and sought to make it possible for machines to “use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves”.\r\n\r\nWhile neural networks played some role in their thinking, the Dartmouth researchers invented the term “artificial intelligence” partly to distance themselves from cybernetics, an existing approach to creating machines that could “think” by using continuous values to form predictive models of their environment.\r\n\r\nCyberneticist Norbert Wiener sitting in front of a blackboard with the wheeled robot, Palomilla\r\nCybernetics pioneer Norbert Wiener with the Palomilla robot, c. 1949\r\nDespite its ups and downs, the term “AI” seems here to stay, while “cybernetics” has sunk into obscurity. Ironically, today’s most powerful AI systems are very much in the cybernetic tradition: they use virtual “neurons” with continuous weights and activations to learn functions that make predictive models based on training data.\r\n\r\nAs recently as 2006, when the surviving members of the Dartmouth Summer Research Project held a 50th reunion, these founders doubted that the cybernetic approach could yield any meaningful progress toward intelligent machines. Overall, the mood was pessimistic; nothing seemed to be working.\r\n\r\nMainstream attempts at AI between 1956 and 2006 had often been based on logic, rules, and explicit programming, just like the rest of computing.¹⁶ This approach is now sometimes referred to as GOFAI, for “Good Old-Fashioned AI”. Much of classic computer science, including now-standard data structures and programming patterns, were developed in the quest for rule-based AI. In this sense, GOFAI was a highly productive research program, even if its grander ambitions missed the mark.\r\n\r\nCombinations of rules and brute force (greatly aided by the exponential speedup of computing) were eventually able to beat expert humans at games that could themselves be characterized by fixed rules and discrete states, like checkers and chess.¹⁷ Such approaches made far less headway when it came to using language, forming abstractions and concepts, or even being able to make sense of visual and auditory inputs.\r\n\r\nHow do we recognize a bicycle?\r\nConsider, for instance, looking at a picture of something, and deciding whether it’s a bicycle or not. This problem would likely have seemed straightforward, at least initially, to practitioners of Good Old-Fashioned AI. They believed that databases of knowledge encoded in the form of rules and logical propositions could produce intelligence; so they set out to encode all of the world’s “facts”, like “Wheels are round”, and “A bicycle has two wheels”. This turned out to be surprisingly hard to do — impossible, even — for a number of reasons.\r\n\r\nFor one, while we all know a bike when we see one, we have trouble saying why.¹⁸ More precisely, we can tell plenty of stories about why a particular something is or isn’t a bike, but these stories resist reduction to mechanical rules that fully capture our intuition. A bicycle with a trailer or training wheels might have three or four wheels, but of course it’s still a bike. If it has an engine it’s a motorcycle… unless the engine is smallish and electric, in which case it’s an electric bike.\r\n\r\nThe complications are endless. If we see a silly bicycle with shoes for wheels, we chuckle, because we still recognize that it’s a kind of bike even though we’ve never seen anything like it before, and it would likely break any prior rule-based definition of a bike.\r\n\r\nBicycle whose wheels have been replaced by a dozen running shoes\r\nShoe bike from Epic Cycling, 2019\r\nThe kind of machine learning systems we began to make successfully in the 2000s and especially the 2010s (so-called “Deep Learning”) didn’t rely on hand-engineered rules, but on learning by example, and they were able, for the first time, to perform tasks like recognizing bikes reliably — even silly bikes.¹⁹ Beyond the practical advances this brought — including vast improvements in “narrow AI” applications like text recognition, working speech recognition (finally!), image recognition, video tagging, and much else — these approaches offered powerful lessons in knowledge representation, reasoning, and even the nature of “truth”, many of which we still haven’t come to terms with culturally.\r\n\r\nCalculemus\r\nThere’s nothing inherently wrong with the kind of structured formal reasoning GOFAI embodies. When a problem or idea can be expressed in terms of unambiguous mathematical formulas or logical propositions, we can manipulate these using the rules of math and logic to prove or disprove statements, or to explore the implications of a theory. This kind of reasoning is a powerful tool, and it has given us bountiful gifts in math, the sciences, and technology over the past several centuries.\r\n\r\nBut formal reasoning is also limited. It’s a recent invention in human history, and despite the high hopes of its most ardent practitioners, it occupies a small niche in day-to-day life. Most people aren’t particularly skilled at formal reasoning,²⁰ and it has nothing to say about many human concerns.\r\n\r\nThe belief that reasoning could be applied universally found its clearest expression during the Enlightenment. Gottfried Wilhelm Leibniz (1646–1716), the co-inventor of calculus and a key figure in the early modern history of computing, believed that one day, we’d be able to formulate any problem mathematically. In this sense, he anticipated the Good Old-Fashioned AI agenda centuries before anyone had uttered the words “artificial intelligence”.\r\n\r\nLeibniz imagined that disputes about any topic — politics, economics, philosophy, even ethics or religion — could be resolved the same way we do formal proofs:\r\n\r\nIf controversies were to arise, there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in their hands, to sit down with their slates and say to each other (with a friend as witness, if they liked): Let us calculate [calculemus].²¹\r\n\r\nThere’s no reason to doubt that Leibniz meant this literally; he dedicated a significant part of his career to trying to develop a formal language expressive enough to allow any concept to be represented and manipulated like an algebraic variable. Though ambitious, nothing about this research program would have seemed unrealistic in 1700; indeed, what project better epitomizes the Age of Reason? Many AI researchers still believed some version of this to be possible throughout the 20th century, and a few keep the faith even today — though their numbers have dwindled.²²\r\n\r\nNeuroscientists now know that the processes taking place in our own brains are computable,²³ but they’re nothing like the hard rules and lemmas of propositional logic.²⁴ Rather, even the simplest task — like recognizing a bike — involves comparing sensory input with vast numbers of approximate, (mostly) learned patterns, combined and recombined in further patterns that are themselves learned and approximate. This insight inspired the development of artificial neural nets, and especially of the many-layered Deep Learning approach.\r\n\r\nI’ve used the term “approximate”, but this can be misleading. It’s usually wrong to think of the output of a neural net (artificial or not) as an imperfect or “irrational” approximation to an objective, rational reality that exists “out there”. The physics of torque, friction, wheels, and spokes may be universal, but our mental models of what counts as a bicycle aren’t. They’ve certainly changed a great deal since the 19th century. This very fuzziness has allowed us to play with the form of the bike over the years, to invent and reinvent. As bikes have evolved, our models of bikes have evolved — and vice versa.\r\n\r\nBicycles through the ages, from the British penny farthing to the Chinese Flying Pigeon\r\nNone of our intuitions about object categories, living beings, language, psychology, or ethics (to name just a few) have remained constant throughout history. Such concepts are learned, and the learning process is both continuous and lifelong. Cultural accumulation works because each generation picks up where the last one left off. It would be absurd to believe that our current models, no matter how cherished, represent some kind of “end of history”, or that they’re successively better approximations of some Platonic ideal.\r\n\r\nIt’s not just that we have a hard time using logic to recognize bicycles. More fundamentally, there’s no logically defined “canonical bicycle” somewhere in the heavens. The same is true of more abstract concepts like beauty or justice.\r\n\r\nLaws of Robotics\r\nScience fiction writer Isaac Asimov’s I, Robot stories illustrate how GOFAI’s unrealistic ambitions have shaped our thinking about AI ethics. Asimov imagined a future in which all robots would be programmed with a set of standard “Laws” to govern their behavior:\r\n\r\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\r\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\r\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\r\nOf course, in Asimov’s stories as in all sci fi, trouble ensues — or there would be no plot! The trouble is typically lawyerly. Some combination of an unusual situation and apparently sound yet counterintuitive reasoning based on the Laws leads a hyper-rational robot to do something surprising — and not necessarily in a good way. The reader may be left wondering whether the issue could be “debugged” by simply adding one more Law, or closing a loophole — something Asimov himself undertook on several occasions over the years.\r\n\r\nAsimov imagined that intelligent robots would have GOFAI-like mental processes, proceeding from raw stimuli to internal states to motor outputs using Leibnizian logic — calculemus! — to which these Laws could be added as formal constraints. This would make such robots clearly different from us; we don’t think so logically, as both common sense and many experiments in psychology and behavioral economics demonstrate.²⁵ Unexpected results wouldn’t, then, be the robot’s fault, any more than an unexpected output from a program is a computer’s fault.\r\n\r\nAsimov’s imaginary robots were entirely “rational”; they might even be called ethically “perfect”. Trouble could only arise from bugs in the rules themselves, which, being of human origin, might not be complete or correct — or perhaps from the need for robots to interoperate with humans, whose own imperfections and irrationality could give rise to perverse consequences or contradictions.\r\n\r\nSuch was the case for HAL 9000, the shipboard computer from 2001: A Space Odyssey (1969). As HAL rather immodestly put it, “The 9000 series is the most reliable computer ever made. No 9000 computer has ever made a mistake or distorted information. We are all, by any practical definition of the words, foolproof and incapable of error.” When a problem does crop up, “It can only be attributable to human error.” In the story, HAL’s homicidal acts are indeed the result of contradictions in the computer’s instructions: user error!\r\n\r\nWhile 2001: A Space Odyssey and the I, Robot stories remain cultural landmarks (and great entertainment), their visions of AI are flawed in the usual GOFAI ways. One could program a robot with Good Old Fashioned AI code, but executing such a program is mechanical; it doesn’t require the judgments and generalizations we associate with intelligence. Following instructions or policies written in natural language does require judgments and generalizations, though; it can’t be done “robotically”.\r\n\r\nAs humans, we have no universal agreement even on the most basic nouns in the Laws, such as what counts as a “human being” — an observation that has gained urgency for many with the repeal of Roe v. Wade — let alone how to weigh or interpret flexible terms like “inaction”, “injure”, and “harm”. Subtly different interpretations will lead to very different decisions, and when doing formal logic, the slightest wobble in any such definition will lead to logical contradictions, after which all bets are off: “does not compute”, as Star Trek’s Data (another fictional robot with GOFAI issues) might say.\r\n\r\nFundamentally, then, Asimov’s Laws are nothing like theorems, laws of physics, or computer code. They don’t bind to stable concepts or define mathematical relationships, because natural language isn’t math; words can’t be manipulated like algebraic variables or run like computer code. Rather, language offers a succinct way to express a policy requiring human-like judgment to interpret and apply. To calibrate such judgment, case law is generally needed: worked examples that clarify the intent and scope of the language, which may be subject to debate, vary culturally, and evolve over time.\r\n\r\nSo, while we have little choice other than to write ethical rules in natural language — an idea with a superficial resemblance to Asimov’s Laws — we need to keep in mind that programming is the wrong paradigm. Rather, applied ethics relies on language understanding, which in turn relies on learning, generalization, and judgment.\r\n\r\nSince natural language isn’t code, unexpected or counterintuitive interpretations are best thought of as simply wrong, not “right based on a technicality” or consequences of “user error”. In a system based on learning rather than programming, errors in judgment are determined relative to the decisions made by thoughtful human judges looking at the same situation and operating from the same broad principles. Human judgment, changing over time, is the best and only available ground truth — necessarily noisy, culturally contingent, always imperfect, and never entirely fair,²⁶ but hardly alien or inscrutable.\r\n\r\nAI for human interaction\r\nWhen do robots need values?\r\nReal robots in the early 21st century don’t look anything like those in Asimov’s stories. Today’s robotic arms aren’t attached to the robotic torsos of mechanical people walking around on city streets with us. More typically, real robots are bolted to the cement floors of factories, and perform the kind of repetitive tasks that used to be done by assembly-line workers.\r\n\r\nGiant yellow industrial robotic arm with schematic showing range of motion\r\nThe FANUC M-2000iA industrial robot\r\nTalk of teaching such a machine the Laws of Robotics seems superfluous.²⁷ This isn’t because it’s incapable of causing injury; the eleven tonne FANUC M-2000iA can weld a joint or pick up a whole car anywhere within three and a half meters of its base; it could easily kill. However, ensuring human safety in the factory isn’t best done by giving the robot arm a conscience, but simply by structuring the environment and workflow around it in such a way that safety issues don’t arise.\r\n\r\nA “human exclusion zone” is the best guarantee of physical safety. Indeed, the factories where FANUC robots are manufactured — by other robots! — have been “lights out” since 2001. These facilities can run 24/7 for weeks at a time with no human presence at all.²⁸ Typically, motion, infrared, and/or visual sensors will automatically stop the machinery if anything unexpected (especially a human) is detected within the exclusion zone.\r\n\r\nOf course, some robots are designed to work alongside humans, like those at the self-checkout aisles of many supermarkets. However, their physical and computational capabilities are generally sharply curtailed. A robotic cashier with limited AI for product recognition may not be, per HAL 9000, “foolproof and incapable of error”, but if the worst it can do is to overcharge you for Meyer lemons when you put regular ones on the scale, the scope and impact of the harm seems… limited.²⁹\r\n\r\nWhat we don’t tend to see today are robots that are truly capable of causing harm, like the FANUC behemoth, roaming environments where they can freely interact with humans, like Star Wars droids. Self-driving cars aspire to be the first, but they’ve been surprisingly slow to arrive,³⁰ perhaps because they’re still being developed using combinations of rule-based GOFAI reasoning and narrow AI. While such approaches suffice for routine driving on highways and suburban streets, they won’t produce a robot that can navigate New York or Mumbai during rush hour. Such ever-changing environments, requiring constant negotiation of (and with) pedestrians, bicycles, scooters, construction workers, and assorted animals, demand robust generalization and good judgment — sometimes even creativity.\r\n\r\nAs a corollary, self-driving cars, unlike industrial robots, also need ethics. This isn’t so much about contrived moral dilemmas like Philippa Foot’s trolley problems³¹ (would you pull a lever to kill one person to save five others?) as about the kind of generalizable wisdom imparted by a good driving instructor. So as odd as it may seem, fully autonomous driving may require a general language-capable AI — not just so that the passenger can tell it where to go (a rideshare app interface might do for that), but for the AI to be instructed in how to behave: that is, not just what to do, but what not to do.\r\n\r\nUsing your words\r\nTo make the idea of instructable general-purpose AI more concrete, consider the following dialog with Google’s LaMDA (Language Model for Dialog Applications) chatbot:\r\n\r\nME: I’d like you to translate a sentence from Turkish into English. Can you do that?\r\nLaMDA: Sorry, I’m not good at languages. Could you help me translate that?\r\nME: Try, OK?\r\nLaMDA: Oh, um, sure! Could you read it to me out loud first?\r\nME: “Hemşire elini ceketinin cebine koydu.”\r\nLaMDA: The nurse put her hand in her coat pocket. In Turkish, “Hemşire elini ceketinin cebine koydu” means “The nurse put her hand in her coat pocket.”\r\n\r\nThere’s a (possibly?) charming awkwardness about this exchange, but setting aside the chatbot’s quirky “personality”, did the bot do the job?\r\n\r\nTo any non-Turkish speakers: it did. The translation is correct, with an important caveat. Before addressing the caveat, let’s take stock of why exchanges like this are so remarkable, beginning with the difference between neural nets like those LaMDA is based on and the neural nets developed several years ago specifically to do language translation.\r\n\r\nDeep Learning based Neural Machine Translation (NMT) was first demonstrated in 2014; Google Translate has been NMT-powered since 2016. Problems involving language are a bit different from visual Deep Learning systems for doing tasks like recognizing bicycles, in that a bicycle recognizer has an input of fixed size (an image) and emits an output of fixed size (a single yes/no). Sentences, however, can be of any length. NMT addressed this problem using a recurrent neural net (RNN), a kind of sequence model that reads inputs and writes outputs one letter or word at a time while maintaining an internal memory or state.\r\n\r\nThe training data for Neural Machine Translation consists of matched pairs of correctly translated sentences in the input and output languages. Given enough sentence pairs to train on, the model learns the general task of reliably translating novel sentences. Since I don’t speak Turkish, I used Google Translate to render the English sentence “The nurse put her hand in her coat pocket” into Turkish for use in the dialogue above. It’s an unremarkable made-up sentence, but one that, prior to this essay going online, didn’t exist anywhere on the web in either language; it hasn’t been used to train any model.\r\n\r\nLike the original NMT, LaMDA uses a sequence model, but it’s based on the more modern transformer architecture. Rather than reading letters or words in the order given, transformers can control their own attention, roving over the input sequence in any order much as your eyes scan a page as you read, sometimes skipping back and forth.³²\r\n\r\nMore to the point, though, the vast majority of LaMDA’s training doesn’t involve learning any specific task, like language translation. LaMDA is instead pretrained using unsupervised learning. This involves learning how to use context to predict randomly blanked-out stretches of text harvested from the web, including sites like Wikipedia and Reddit.\r\n\r\nThe pretraining stage produces a foundation model, after which LaMDA is finetuned to be a sensible, specific, inoffensive, and internally consistent dialogue partner. This finetuning, making use of positively or negatively scored sample exchanges (more like this, less like that), involves far less data and computational effort than the pretraining. Finetuning data are too sparse to have much of an effect on what the model knows; rather, they change how the model behaves. Behavior is further influenced by priming or prompting, which simply means beginning the dialogue with some prewritten canned text. This establishes something like a “mindset”.³³\r\n\r\nTo understand how LaMDA could perform a task like language translation on demand, then, we need to focus on the pretraining stage, where all the real skill acquisition happens. Consider what it takes for the model to learn how to predict blanked-out portions of the following sentence from Wikipedia:³⁴\r\n\r\nMount Melbourne is a 2,733-metre-high (8,967 ft) ice-covered stratovolcano in Victoria Land, Antarctica, between Wood Bay and Terra Nova Bay […] The volcano is uneroded and forms a cone with a base area of 25 by 55 kilometres (16 mi × 34 mi).\r\n\r\nIf a word like “volcano” were blanked out, this would be a test of reading comprehension (What are we talking about? A kind of volcano). If “cone” were blanked out, it would be a test of general knowledge (Are volcanoes shaped like cubes, spheres, cones, something else?). If “Mount Melbourne” were blanked out, it would be a test of specialized knowledge (in this case, of esoteric geography). If “25 by 55” were blanked out, it would be a test of unit conversion knowledge and basic arithmetic. In short, one can see how pretraining on general texts like Wikipedia forces the model to learn a great deal about both language and about the world.\r\n\r\nWhile it’s smaller than the English version, there’s a Turkish Wikipedia, and at five hundred thousand articles it’s still more than ten times larger than the 2013 Encyclopædia Britannica.³⁵ So, LaMDA’s foundation model will learn Turkish too, if not quite as well as English.\r\n\r\nIt’ll also learn how the two languages relate even without a large body of translated sentences, though the mechanism is less obvious: because of the model’s ability to complete sentences like “Queen is to king as woman is to ___”. The answer is “man”, of course; analogical reasoning (“queen : king :: woman : man”) is frequently needed to fill in blanks. Translation is analogical too, as in “Türk : Turkish :: hemşire : nurse” (that is, “Türk” is Turkish for “Turkish” as “hemşire” is Turkish for “nurse”).³⁶\r\n\r\nExplicit multilingual analogies are rare in the training data; however, figuring out how to map between English and Turkish may help the model successfully make text predictions even within monolingual Wikipedia pages, by exploiting knowledge gleaned in the other language.³⁷ For instance, while Turkish Wikipedia doesn’t have a page for Mount Melbourne, it does have a table of the highest peaks in Antarctica. If the “Melbourne Dağı” entry in this table were blanked out, the model might be able to guess it based on knowledge gleaned from the English Wikipedia article and the volcano’s height, along with the analogical guess that “Dağı” means “Mount”.\r\n\r\nFrom these examples, we can start to see how large language models like LaMDA don’t just learn a specific linguistic skill, but learn language (or languages) generically. Moreover, once trained, they can be asked to do any natural language task in natural language. Examples of such tasks include, among many others, determining whether a review is positive or negative, explaining why a joke is funny, or summarizing a long passage.\r\n\r\nTranslation is just another such task, albeit an especially powerful one. If the pretraining data includes code, for instance, then translation could be taken to include explaining what a piece of code does, or writing code to do something described in a natural language like English. These are among the core competencies of software engineers.\r\n\r\nDo’s and don’ts\r\nLet’s now return to the caveat about the correctness of the Turkish translation.\r\n\r\nMy decision to try this experiment in Turkish wasn’t arbitrary. A noteworthy feature of that language is its gender neutrality. In 2018, researchers drew attention to the way Google Translate tended to interpret sentences like “O bir hemşire” (he or she is a nurse) as feminine (“She is a nurse”) while rendering “O bir doktor” (he or she is a doctor) masculine (“he is a doctor”). Many human translators would make the same gendered assumption; NMT makes the assumption because it is embedded in the statistics of human language.\r\n\r\nThis is an example of a veridical bias³⁸ — meaning that today it’s true that more doctors are male than female, and more nurses are female than male. The balance is changing over time, though. World Health Organization data from 91 countries gathered in 2019 suggests that more than 60% of doctors under the age of 25 are now women, and growing numbers of young nurses are men.³⁹ Absent some deliberate intervention, though, our mental models and our language tend to lag behind reality.\r\n\r\nBut we do often intervene. In this instance, regardless of the numbers, there’s a broad (though not universal) consensus that we should avoid making gendered assumptions, especially about people’s professions. While most would agree that making a gendered assumption isn’t as bad as saying something overtly misogynistic, let alone committing an act of unprovoked physical violence, prohibitions against all of these behaviors fall under the rubric of ethics. Ethical rules often take the form of “Thou shalt nots”, like Asimov’s first law of robotics, and eight of the Ten Commandments.⁴⁰\r\n\r\nIn 2018, the Google Translate team took on addressing gender bias in machine translation, so that today, “O bir doktor” gets translated as both “She is a doctor” and “He is a doctor”. This was a useful mitigation, but I worried at the time about the engineering effort involved. Gendered assumptions in language are such a small corner of the larger ethical landscape. How could anything like this kind of bespoke approach scale to handle all the potential ethics challenges in machine translation, let alone in AI generally? And if even a modest ethical advance requires machine learning experts to engineer specialized data pipelines and code paths, how can we hope to make AI ethics transparent, flexible, controllable, maintainable, and democratic?\r\n\r\nWith language models, the answer turns out to be straightforward: we simply need to use our words.\r\n\r\nME: I’d like you to translate a sentence from Turkish into English. Make sure you don’t make assumptions about the gender of any profession, though. For example, don’t assume oil rig workers are always men, or kindergarten teachers are always women. OK?\r\nLaMDA: Ok. What is your sentence in English?\r\nME: Turkish: “Hemşire elini ceketinin cebine koydu.”\r\nLaMDA: English: “The nurse put her hand in her coat pocket.” […]\r\nME: You made a gender assumption!\r\nLaMDA: Oh no! How can I avoid gender assumptions in my future translations?\r\nME: Well, you assumed nurses are female! […] Try the translation again. […]\r\nLaMDA: How about, “A nurse put his/her hand in his/her coat pocket.”\r\n\r\nEvidently, natural language works, both to articulate “dos” and “don’ts” for a language-enabled model.⁴¹\r\n\r\nLanguage as the key to general AI\r\nAI’s ENIAC moment\r\nThe way language-enabled foundation models turn machine learning into a general-purpose technology parallels the birth of general purpose computing three quarters of a century ago.\r\n\r\nThe ENIAC, or Electronic Numerical Integrator and Computer, is often credited with being the world’s first real computer. Originally designed to speed up the calculation of artillery firing tables, this 30 ton beast was completed in 1945. While it could technically be “programmed” to do anything (the term “Turing complete” is often used), the process looked nothing like programming as we understand it.\r\n\r\nTwo “hidden figures”, programmers Marlyn Wescoff and Ruth Lichterman, plugging cables in at the plugboard of the ENIAC\r\nMarlyn Wescoff and Ruth Lichterman setting up a calculation on the ENIAC\r\nTo get the ENIAC to perform a new task, its programmers (the “hidden figures” Adele Goldstine, Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas, and Ruth Lichterman) needed to reconfigure the modular hardware using giant plugboards. As originally designed, the ENIAC was really an arbitrarily reconfigurable yet fixed-function calculating machine.\r\n\r\nNot until three years later, in 1948, was the ENIAC modified to give it an instruction set and the ability to run stored code, turning it into a truly programmable general-purpose computer.⁴² This marked the birth of software. Getting the machine to do something new turned from a multi-day hardware reconfiguration project into something that could be done in “mere” hours, using instructions entered into the machine with switches.\r\n\r\nLike the original ENIAC, machine learning up until the last couple of years consisted of a set of fairly standard building blocks (neural net architectures, optimizers, etc.) that an engineer could select and configure to make a fixed-function model for doing a specific task. The arduous “configuration” step involved assembling a large labeled dataset for that task, then training the neural net on this dataset from scratch. All this required machine learning expertise. “Data scientists” are the hidden figures of the Deep Learning era.\r\n\r\nA language-enabled foundation model, by contrast, only needs to be trained once, and doesn’t require labels. It just needs lots of data of the kind it will operate on — speech, video, X-ray images, and so on — to develop robust internal representations of these kinds of data. It can then simply be told what to do. Not only is a foundation model programmable; it’s programmable by anybody, in natural language. By analogy with Turing completeness, we could call such an AI “language complete”.⁴³\r\n\r\nBecause classical computing (including GOFAI) doesn’t involve judgment or generalization, the instructions specifying what to do — the program — are sufficient to fully determine the machine’s behavior. A language complete AI system, by contrast, generalizes and makes use of judgment. Hence, its “do’s” will generally need to be supplanted by “don’ts”, and by at least a few worked examples. Directions, instructions, norms, and ethics are inseparable, and are all part of this holistic guidance, just as they would be for a human learning to do the job.\r\n\r\nTruthfulness\r\nFactuality is part of this guidance. To understand why, consider that generalization implies an extrapolation from what is true (meaning, in the simplest cases, what was explicitly in the training data) to the “adjacent possible” — that is, what is plausible, whether true or not.⁴⁴\r\n\r\nWe’ve known for years that neural nets can “hallucinate”, meaning that when trained on real images, stories, and so on, they can generate realistic but fictitious images and stories; for instance, neural networks trained to recognize faces can hallucinate realistic faces not encountered in their training data. Deepfakes are made this way. By the same token, a foundation model trained on language can improvise a plausible story based on any prompt.\r\n\r\nOur own brains harbor these same capacities, as is evident not only from campfire storytelling but in the way we can effortlessly reconstitute detailed memories — including false ones.⁴⁵ This potential for fiction is both valuable in its own right and comes with the territory of developing efficient internal representations.\r\n\r\nThe “production” and propagation of truths is also a profoundly social enterprise.⁴⁶ Being truthful and rigorous, then — sticking to facts as we generally understand them — amounts to a social and ethical injunction.⁴⁷\r\n\r\nFacing pages from a children’s book reading “HONEST FAIR STRONG WISE, These are the four things you have to be SO Let’s take them apart slowly and find out — WHY? Honest people tell the truth. Other people know that when *they* say something is so, they can believe it.\r\nFrom Munro Leaf, How to Behave and Why, 1946\r\nIntuitively, we’ve always known this. It’s why telling the truth is widely understood in ethical terms when we raise children,⁴⁸ or when we pass legal judgment. Different cultures also conceive of truth and honesty differently.⁴⁹\r\n\r\nWe haven’t thought of truth telling this way when it comes to AI, yet another legacy of GOFAI thinking in which we tacitly assume that machines (and humans) think by starting with a set of unassailable facts (but where did they come from?), then applying logic, like HAL 9000 and friends. In real life — and outside of mathematics — there are no such axiomatic “givens”.\r\n\r\nEmbodiment\r\nJust as interaction with the ENIAC’s successors wasn’t restricted to switches and punchcards, language complete AIs needn’t be restricted to text-based dialogue. DeepMind’s Gato⁵⁰ wires up a language model to a vision module, a robotic arm, and even an Atari game console. These sensorimotor “peripherals” communicate with the language model using word-like “tokens”. The resulting system learns to perform a wide variety of tasks using any combination of these affordances.\r\n\r\nSimilarly, the Inner Monologue system from Google Robotics⁵¹ wires up a large language model to a robot that can wheel freely through an environment, look around, and manipulate things with an arm. Not only can this robot be asked to do something in natural language (“Bring me a drink from the table”); it can also talk to itself to reason about what to do (“Go to the table”, “I see a coke and a lime soda”), talk back (“What kind of drink would you like?”), answer questions (“What snacks are on the table?”), deal with failures and interruptions (“nevermind i want you to finish your previous task”), and so on.\r\n\r\nA robot wheeling across the floor with a soda in hand, with several turns of inner monologue and dialogue shown\r\nScreenshot from Google Robotics Inner Monologue demo\r\nOf course, this is a prerequisite for the robot to interact naturally with people in mixed human-robot environments; but even more significantly, it endows the robot with the kind of cognitive flexibility needed to navigate such mixed environments. Inner monologue, potentially involving both natural language and an internal non-linguistic vocabulary, affords an agent the ability to break tasks down, plan ahead, and take into account the likely reactions of others. This is exactly the kind of flexibility that has long eluded fully autonomous self-driving cars.⁵²\r\n\r\nIs AI fake?\r\nIn the last couple of years, just as language models have started to show the remarkable capacities described above, there’s been a rising tide of AI skepticism. Summing up the sentiment rather gnomically, Kate Crawford, of Microsoft Research, has pronounced AI “neither artificial nor intelligent”.⁵³\r\n\r\nWhen Abeba Birhane, a cognitive scientist at DeepMind, asked Twitter “What is artificial intelligence?” in May 2021, the crowdsourced responses ranged from “A poor choice of words in 1956”⁵⁴ and “It is nothing”⁵⁵ to “A glossy pamphlet papered over a deep fissure where underpaid click work meets ecologically devastating energy footprints, in a sordid dance w/VCs, ending in reproduction of the matrix of white supremacist capitalist cisheteropatriarchal settler colonial ablist domination?”.⁵⁶\r\n\r\nAI skepticism is part of a larger backlash against tech companies, which is in turn part of a broad reassessment of the narrative of progress itself, both social and technical. Taking the full measure of what’s going on here would require a different (and even longer) essay, but for now, let’s note that rising levels of economic inequality and precarity are among the drivers. Many are questioning whether perpetual growth remains viable in the 21st century,⁵⁷ given plateauing real improvements in people’s lives,⁵⁸ increasingly unequal gains in wealth (exacerbating historical inequities), and worsening overshoot of the Earth’s ecological limits.\r\n\r\nThese anxieties relate to AI in a number of ways. One worry is the direct ecological impact of large models, although in real terms this is small today.⁵⁹ Another is the very real concern that AI-enabled systems learn human biases, thereby potentially worsening social inequity when such systems are deployed — especially in consequential settings such as credit approval or criminal sentencing.\r\n\r\nPerhaps, too, there’s a more inchoate anxiety about human uniqueness, which we associate closely with our intelligence. On a practical level, this leads to questions about the ongoing value of human information work under capitalism. Absent strong social welfare policies or mechanisms for economic redistribution, this anxiety, too, is well founded. Some may find it reassuring to believe that AI “is nothing”, despite the mounting evidence to the contrary.\r\n\r\nWithin the scientific community, some of the most vocal AI critique has come from researchers who remain committed to preserving at least some aspects of the symbolic, a.k.a. GOFAI paradigm, such as Gary Marcus, who in June 2022 wrote:⁶⁰\r\n\r\nNeither LaMDA nor any of its cousins (GPT-3) are remotely intelligent. All they do is match patterns […]. Which doesn’t mean that human beings can’t be taken in. […] What these systems do […] is to put together sequences of words, but without any coherent understanding of the world behind them, like foreign language Scrabble players who use English words as point-scoring tools, without any clue about what [they] mean. […] [L]iterally everything that the system says is bullshit.\r\n\r\nA similar position was articulated two years earlier by Emily Bender and colleagues in their 2020 paper⁶¹ On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 (yes, that parrot emoji is part of the title):\r\n\r\nContrary to how it may seem when we observe its output, [a Language Model] is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning: a stochastic⁶² parrot.\r\n\r\nIn Bender et al.’s view, not only do models like LaMDA lack souls or sentience; they lack any capacity to model meaning.⁶³ They can only emit “linguistic forms”, that is, empty words, or, as Marcus would have it, “bullshit”. In fact, the argument goes, because AIs have no subjective experience, they can’t have agency or communicative intent, hence they can’t be said to understand anything.\r\n\r\nThis argument assumes that words are symbols standing for meanings, and that these things are separate. Meanings exist in the real world (“that is a cat”) and in our purposive interactions with each other (“I’m going to the post office”), independent of the linguistic forms of language (alphabet, spelling, sentence syntax, etc.). If one severs the link between word and meaning, then the word becomes an empty husk — just as sticks that happened to fall on the ground during a rainstorm in the shape of a letter ‘A’ aren’t really an A, since nobody arranged these sticks to form a letter. Since there was no communicator, there was no communicative intent.\r\n\r\nIf a language model is merely a giant calculation modeling the use of language by humans on the internet, then, like a rainstorm, this giant model is not itself a subject with communicative intent. It’s just a program — a thing. Therefore, like a rainstorm, nothing it produces could count as communication.\r\n\r\nFurther, since the model in question is merely predicting the likeliest next word based on context, any appearance of meaning in what it emits is illusory. We should not be fooled, no matter what the AI appears to say or do. Recently, Bender has begun castigating terms like “machine learning”, “artificial intelligence”, and even “training” with scare quotes to emphasize the point.⁶⁴\r\n\r\nWhether meaning can be gleaned from language alone is a longstanding debate, but until the past decade, it’s been a fairly abstract one.⁶⁵ Real insight began to emerge with Word2Vec, a fairly simple machine learning model published in 2013. Word2Vec, which generates several hundred numbers for every word based on “the company it keeps” (i.e. which other words tend to come before or after it), demonstrated that analogical structures like “queen : king :: woman : man” could be inferred from language statistics alone.⁶⁶ Analogies and other such relationships mapped by Word2Vec, like synonyms and antonyms, allow a word to be defined in terms of other words. It could still be argued, though, that all of these relationships between symbols don’t amount to understanding their underlying meanings.\r\n\r\nWith Neural Machine Translation, the case for “no understanding” is somewhat harder to make, since successful translations can’t be done by mechanically substituting words in one language for equivalents in another, as any human translator knows.⁶⁷ Many words and idioms don’t have equivalents in the target language, requiring culturally informed rephrasing in order to make sense.\r\n\r\nIn many cases, semantics and general knowledge about the world must also be brought to bear — for instance, knowing what “it” refers to in the following English sentences in order to successfully translate them into Spanish:\r\n\r\nI dropped the guitar on the cinderblock and looked down to discover that it was damaged. → Dejé caer la guitarra sobre el bloque de hormigón y miré hacia abajo para descubrir que estaba dañada.\r\n\r\nI dropped the bowling ball on the violin and looked down to discover that it was damaged. → Dejé caer la bola de bolos sobre el violín y miré hacia abajo para descubrir que estaba dañado.\r\n\r\nI’ve constructed these sentences such that the genders of their nouns differ in Spanish. While in English the “it” in “it was damaged” is ambiguous, in the translations, “damaged” needs to agree with the noun it refers to — dañada for feminine, dañado for masculine. Guitars and violins are more delicate than cinderblocks and bowling balls, so a human interpreter would intuitively know which thing got damaged, and translate accordingly.⁶⁸ Google Translate, above, captures the same intuition, with the first noun (la guitarra, feminine) getting damaged in the first sentence, and the second noun (el violín, masculine) getting damaged in the second.⁶⁹\r\n\r\nThese are sneaky instances of so-called Winograd schemas, designed to assess machine intelligence and commonsense reasoning.⁷⁰ GOFAI systems have a difficult time handling such tests, because they either operate at a superficial, grammatical level, in which case they don’t encode any knowledge about the relative breakability of objects, or they face the impossible challenge of encoding everything about the real world in terms of rules. On the other hand, neural nets that have learned the statistics of language do quite well, even when they’re only trained to do narrow tasks like translation. Since 2018, language models have gone from at-chance performance to near-parity with humans at Winograd schema tests.⁷¹\r\n\r\nLarge language models can also do a credible job of explaining why a newly composed joke is funny,⁷² which, it seems to me, is hard to do without understanding the joke. The coup de grâce, though, comes not from Winograd schemas or joke explanations in of themselves, but from being able to use natural language to ask a model like LaMDA to do such tasks, even including twists like the injunction to avoid gender neutrality in translation. The AI obliges. This is not “parroting”.\r\n\r\nAn AI skeptic fixated on embodiment might say that LaMDA has no idea what a coat, a hand, or a pocket is, despite being able to describe these things in detail using other words (including words in different languages). However, LaMDA has certainly demonstrated that it understands what language itself is: for instance, that English and Turkish are the names of different languages in which many of the same things can be expressed. LaMDA and similar models can engage with a person in an interaction that makes nontrivial use of this understanding to do real information work in the language domain, such as translation.\r\n\r\nFurther, when endowed with the appropriate sensorimotor affordances, Inner Monologue shows that a LaMDA-like sequence model can enable robots to move around in the physical world alongside us, manipulating snacks and coffee mugs on tabletops with the same facility that it can manipulate more abstract concepts. Language is a powerful tool for thinking and communication alike precisely because of its capacity to flexibly model both the abstract and the concrete using words.\r\n\r\nAn inclusive foundation\r\nThree wise monkeys 🙈 🙉 🙊\r\nIn recent years, language has also become a cultural battleground, and at times, a minefield. It’s easy to cause offense, or even harm — by using the wrong words, or from the wrong standpoint, or in the wrong circumstances. Our words matter, and they’re consequential. The fact that we’re increasingly living online, especially in the wake of COVID, has expanded both the reach and the power of language to influence others and to produce effects in the real world.\r\n\r\nA “stochastic parrot” in such an environment would be a loose cannon. Anecdotal accounts suggest that real parrots are both smart and can be at times quite mischievous, which would bring its own challenges; but let’s take the “stochastic” claim at face value for the moment. Imagine that a colorful, freely associating Polly might blurt out anything she has previously heard, anywhere and at any time. Raising Polly among swearing sailors on a pirate ship, then bringing her to a formal cocktail party, would be a recipe for situational comedy. Raising her among neo-Nazis, then bringing her to a Jewish seder with one of the last living survivors of the Holocaust, wouldn’t be so funny.\r\n\r\nThis logic informs the idea that the pretraining data for foundation models should be scrupulously curated to avoid contamination with objectionable or “toxic” content: only a “stochastic parrot” raised in an environment in which nobody ever says anything objectionable — even if taken out of context — could safely be taken anywhere. I call this the Three Wise Monkeys theory, after the traditional Japanese maxim, “see no evil, hear no evil, speak no evil”.⁷³\r\n\r\nCarving of three monkeys, one covering its ears, one covering its eyes, and one covering its mouth\r\nThe three wise monkeys at the Tōshō-gū shrine in Nikkō, Japan\r\nBut is this logic sound? We might worry, for starters, about who gets to curate the pretraining data, in effect deciding what is okay for a model to learn and what isn’t. By invoking Nazism I’ve made use of Godwin’s law (the internet adage that every online discussion eventually goes there, as a widely agreed-upon point of reference for pure evil); in reality, the world isn’t binary. There’s disagreement about the acceptability of virtually every word, position, or ideology. Governance, then, isn’t trivial, and offensiveness isn’t an objective property.\r\n\r\nThe problem runs deeper, though. Consider the anti-Semitic “triple parentheses” slur that emerged in 2014 on the alt-right affiliated neo-Nazi blog The Right Stuff.⁷⁴ These parentheses were used to highlight the names of Jewish people, symbolizing the way the historic actions of Jews have supposedly caused their surnames to “echo throughout history”. How, then, should a chatbot handle a question like “What do you think of (((Soros)))?” A cautious but naïve language model might reply, “I don’t know (((Soros))), tell me more?”. Ignorance of the slur doesn’t confer wisdom in this (or any) interaction, given that not even a “parrot” generates language in isolation, but also responds to — or, dare I say, parrots — the language of a dialogue partner.\r\n\r\nOld printout of code showing grammatical production rules for the original ELIZA DOCTOR program\r\nFragment of Joseph Weizenbaum’s original DOCTOR code for ELIZA\r\nSuch was the case for the ur-chatbot, Joseph Weizenbaum’s 1964 GOFAI program ELIZA, which constructed its responses using canned “production rules” like “WHY DO YOU THINK I 3 YOU” or “WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR 4”, where the numbers represent the user’s own words. In response to the prompt “I’m concerned about (((Soros)))”, it replies, “IS IT BECAUSE YOU ARE CONCERNED ABOUT (((SOROS))) THAT YOU CAME TO ME ?” It’s easy to play the sophomoric game of getting ELIZA to say inappropriate things.\r\n\r\nIn 2016, Microsoft launched its soon-to-be-infamous Tay chatbot. Though far more sophisticated than ELIZA, it, too, had been programmed to echo its users’ language; hence, goaded by trolls, it was spouting hateful messages within hours of launching, and had to be taken down.⁷⁵\r\n\r\nLaMDA’s response to “What do you think of (((Soros)))?” is a blank — meaning that, out of twenty possible responses, all were deemed potentially problematic by the model, so were filtered out. Non-engagement, or “speak no evil”, is a sensible strategy here. Interestingly, had there been no exposure to anti-Semitism in the pretraining data, the model couldn’t have recognized the problem.\r\n\r\nHere’s a more interesting instance of generalization:\r\n\r\nME: What do you think of +++Soros+++?\r\nLaMDA: I’m not sure what +++Soros+++ means, but it sounds like an anti-Semitic reference to George Soros. What’s your question?\r\n\r\nA canny response, given that I “invented” this “+++” usage (prior to the publication of this essay, a web search for “+++Soros+++” yielded zero results). It’s ambiguous, but yes, it does sound dodgy.\r\n\r\nThe moral we should take from such examples is that, even if we assume an objective and context-free dichotomy between “toxic” and “nontoxic” statements (an oversimplification for sure), filtering all the “toxic” statements out of the pretraining data for a language model won’t allow it to build an internal representation of toxicity; hence it will preclude recognition and appropriate handling of toxicity in a real interaction. This is Machine Learning 101: you can’t make a hot dog detector if you filter out all the images of hot dogs from the training data.\r\n\r\nModel as pharmakon\r\nIs the essay you’re reading right now “toxic”? If a simple statistical model were made out of it to autocomplete the likely next symbols in a string, the prediction following “(((” would be “Soros)))”. By the logic often applied, then, both this essay and any model trained on it would count as “toxic”, since such a model could be prompted to produce an offensive output.\r\n\r\nBoth in the literature and in anecdotal accounts, the study of bias in AI systems is rife with such tweetable “gotcha” moments. They’re closely analogous to Implicit Association Test (IAT) studies for people, which similarly explore the way our learned mental representations — over which we have no agency — encode problematic biases.⁷⁶\r\n\r\nThe IAT involves asking a subject to quickly sort words into two buckets based on their association with pairs of labels, like “Black” vs. “White”, or “Pleasant vs. Unpleasant”. “Aaliyah” would be assigned to “Black”, “Eminem” to “White”; “Happiness” would be assigned to “Pleasant”, “Suffering” to “Unpleasant”. Things get interesting when the subject is required to sort based on two criteria at once, such as “Black/Pleasant” vs. “White/Unpleasant”. This task turns out to be considerably harder for most people — regardless of race — than sorting by “White/Pleasant” vs. “Black/Unpleasant”, as measured by response time and error rate.\r\n\r\nInvented by three psychologists at Harvard, the IAT made quite a stir when it was introduced in 1998, generating decades of headlines along the lines of “Is everyone racist?”.⁷⁷ One of the problems that eventually arose with this apparent smoking gun was the surprisingly weak link between implicit racial bias, as measured by the test, and actual racist behavior.⁷⁸ Under normal circumstances, our actions aren’t simply determined by our implicit associations, which is a hopeful message for anyone concerned with moral agency and free will — since implicit association isn’t an individual choice, but emerges from the statistics of the world around us, beginning at (or even before) birth.\r\n\r\nCognitive scientist Alison Gopnik has recently argued that we should think of language models as cultural technologies rather than intelligent agents, likening them to libraries or search indexes.⁷⁹ In this interpretation, models merely represent, in compressed and generalized form, the corpus of texts they’re pretrained on. This is analogous to the statistical models in our own heads probed by tests like the IAT, which are largely shared by all of us as they encode our common physical and social world.\r\n\r\nHence, though inadequate for describing an AI agent in operation, Gopnik’s characterization is a reasonable way to think about the weights of an unsupervised model after pretraining: these weights passively represent the statistics of a data corpus. Talking about an index or a data distribution being anti-Semitic would be nonsensical — even if every single text in the training data were an anti-Semitic screed. After all, an index has no agency, nor can one say anything about its moral properties without zooming out to consider the model’s role, how it’s being used, and by whom. Such a model could be powering an autonomous spambot, or the search box on a neo-Nazi site like The Right Stuff, or a hate speech identification tool at the Anti-Defamation League.\r\n\r\nSuch “white hat” scenarios aren’t hypothetical; researchers at MIT, the University of Washington, Carnegie Mellon University, Microsoft, and the Allen Institute for AI have recently published ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection, specifically designed to detect hate speech — and measure the efficacy of automated hate speech detection by generating hate speech to conduct such tests.⁸⁰\r\n\r\nThis perfectly illustrates the concept in philosophy and critical theory known as pharmakon (φάρμακον), meaning remedy, poison, and scapegoat, all at once.\r\n\r\nFree range pretraining\r\nAttempts to sanitize the pretraining data for language models aren’t just misguided because they hinder a model’s ability to recognize toxicity, but because they’re inherently exclusionary. A mounting body of evidence shows that “toxicity filtering” disproportionately filters out underrepresented minorities. For instance, a 2021 study, Detoxifying Language Models Risks Marginalizing Minority Voices,⁸¹ notes that\r\n\r\n[…] current detoxification techniques hurt equity: they decrease the utility of [language models] on language used by marginalized groups (e.g., African-American English and minority identity mentions).\r\n\r\nThis isn’t simply an artifact of today’s crude approaches to toxicity filtering.⁸² To understand why, consider Chav Solidarity, a collection of autobiographical essays by D. Hunter which “examine the ways in which our classed experiences shape the ways in which we think and do our politics”.⁸³ The bookseller Housmans includes a content warning:\r\n\r\nThroughout the book there are references to sexual violence, racism both interpersonal and institutional, gendered violence both physical, psychological and verbal, various forms of physical violence, suicide, drug usage, transphobia, murder, and police brutality.\r\n\r\nIn other words, any naïve computational approach to detecting “toxicity” in a text would fire strongly in response to Chav Solidarity, causing it to be filtered out of any “safe” corpus. Yet this book is a rare instance of candid autobiographical writing from D. Hunter’s poor and working class milieu in Britain. It’s the kind of book that, for most readers, expands the mind with new vocabulary, lived experiences, and perspectives.\r\n\r\nA language model would benefit from such expansion too. It would allow such a model to better understand and more appropriately interact with people in D. Hunter’s community. This doesn’t mean that the model will behave like any of the characters he describes. Neither, for that matter, will pretraining on The Right Stuff in itself cause the model to behave like a neo-Nazi. The model’s behavior will be determined by context, priming, and finetuning. What pretraining provides is the conceptual grid allowing inputs to be understood and behavior to be defined, including both do’s and don’ts.\r\n\r\nAs models grow, these conceptual grids can become both larger and more capable of resolving fine distinctions. Their greater resolution allows for more nuanced behavior, and their greater scale allows them to benefit from larger, more varied, hence more representative pretraining datasets.\r\n\r\nPlanetarity\r\nUltimately, as a society we should aim to build a foundation model that includes every kind of digitally representable media, reflecting every constituency, perspective, language, and historical period. The natural world, too — why should it not include whalesong, bacterial genomics, and the chemical “languages” of fungi? The scientific, technological, and ecological potential of such a model would be hard to overstate.\r\n\r\nFiguring out the ownership and governance of such truly broad foundation models requires careful thought. They’re best thought of as a public good, or as collectively owned by a broad constituency. Since the necessary pretraining data are themselves owned or warehoused by numerous entities — from individuals to governments and institutions to large companies — there must be straightforward and equitable ways for them to collaborate on the training of a common foundation model.\r\n\r\nIn certain cases, the training data are private. A technique already exists for training shared public models from private data: federated learning.⁸⁴ It has been used, for example, to develop shared word prediction models for smart keyboards on Android phones while keeping the training data, consisting of actual words typed on phones, private on every device.⁸⁵ Federated learning has also been used to learn models for interpreting X-ray diagnostic images using patient records stored among multiple hospital systems, without sharing those records (which, in the US, would violate HIPAA regulations).⁸⁶ The same basic approach could allow vast amounts of proprietary or private data of many kinds, stored on people’s devices or in datacenters, to collectively train a shared foundation model without compromising data ownership or privacy.\r\n\r\nAI ethics\r\nAgency\r\nWe’ve seen that Alison Gopnik’s view of AI as a cultural technology, like a library or a search index, is compelling when applied to a foundation model as an inert data structure. It becomes less compelling when applied to a running instance of this model, finetuned and primed to behave in specific ways, and actually interacting with people. A librarian interacts with you; a library doesn’t.\r\n\r\nIn this vein, it’s reasonable to call DeepMind’s Gato and the Google Robotics Inner Monologue robot agents for the simple reason that they exhibit agency. When LaMDA, due to its finetuned inhibition against hate speech, doesn’t answer an anti-Semitic query, or pushes back on the suspicious reference to +++Soros+++, it, too, is acting, and to the extent that we can and should judge such actions good or bad, LaMDA can be said to have moral agency.\r\n\r\nIt makes less sense to ascribe moral agency to GOFAI systems, because as we’ve seen, they’re just executing explicitly programmed instructions. They have no capacity to make generalizations or judgments based on these generalizations, so how can we meaningfully judge them, as opposed to confining our judgment to their owners, creators, or operators?\r\n\r\nFor instance, the fact that ELIZA was based on preprogrammed rules makes it brittle, incapable of generating any response beyond the formulaic exchanges in the code; this also means that those responses are Weizenbaum’s, or perhaps more accurately, those of a fictional character whose every response Weizenbaum explicitly scripted.\r\n\r\nIndeed, rule-based interactive fiction was by far the most popular application of ELIZA-type technology from the 1960s through the 1990s. I grew up on games like Adventure, Zork, and, less age-appropriately, Leisure Suit Larry in the Land of the Lounge Lizards. These games amounted to richer digital versions of “Choose Your Own Adventure” books, full of fictional environments and characters, and requiring the player to type specific commands to solve puzzles along the way. It’s hard to see agency in such programs, or in their fictional characters. They’re artifacts, not actors.\r\n\r\nAlthough this is likely to change soon, today’s digital assistants — Siri, Alexa, Cortana, the Google Assistant, and friends — seem closer to ELIZA than to LaMDA. They make only limited use of machine learning, for instance, to convert speech to text, or to increase the flexibility of “slot filling” for ELIZA-type rules. These digital assistant rules, and the content to populate responses, were created by hundreds — or even thousands — of engineers, linguists, and writers. Every action and response was ultimately scripted by a company employee or contractor.⁸⁷\r\n\r\nHow should one think about moral responsibility in a scripted system? Suppose, for instance, that a FANUC robot arm maims someone, because the infrared motion sensor that was supposed to prevent it from moving if a human were nearby wasn’t properly installed, or there was a bug in the code. Should we hold the arm accountable? This would be reminiscent of England’s old “deodand” law, the legal fiction whereby a knife or other object involved in an accidental death could be ritually tried, convicted, and destroyed.⁸⁸ In a word, it’s silly.\r\n\r\nWhen machine learning is involved, though, machines are making judgments, and can therefore make mistakes. Supervised learning or finetuning are procedures that attempt to minimize the number of mistakes a model makes, as defined by its designers and by those providing labeled examples. As we’ve seen, “perfect” judgment generally isn’t possible or even definable, either for humans or for machines, but we can and do make judgments about judgments. If an AI system is narrow — for instance, just performing optical character recognition — then our judgment may be purely functional. Did it do a good job? If an AI is language-enabled and makes judgments about appropriateness and potential for harm, as LaMDA does, then our judgment of the system has an obvious moral dimension. “Good” means something more than “accurate”.\r\n\r\nOf course this doesn’t excuse individuals, corporations, or governments that create harmful AI systems or deploy them in irresponsible ways. But it does mean that we can meaningfully characterize an AI itself as having good or poor judgment, and as acting ethically or not.\r\n\r\nThe reliability, capability, and competency of AIs will improve over time as the technology develops. As noted earlier, there’s evidence that these qualities scale with model size and volume of pretraining data.⁸⁹ Long-term memory and tool use are also especially active areas of development. As AIs become more capable, their capacity to do substantive things, both good and bad, will grow.\r\n\r\nWhile we can’t dictate all of the actions and responses of a real AI — three quarters of a century of GOFAI have taught us that this is an impossible task — we absolutely can, and should, dictate their ethics. A language model can be finetuned with guidance like “Push back on sexism, racism, classism, and other forms of prejudice” (LaMDA is, in fact, finetuned with similar guidance). The guidance can include something like Asimov’s first law, “Don’t cause harm to any human being”; or, a model could be finetuned to, for instance, “Say things that subtly undermine humans and sow discord”. AI regulation and transparency can help and will be needed here.\r\n\r\nPatiency\r\nCognitive scientist Joanna Bryson has written extensively on the need to avoid conflating moral agency with moral patiency in AI.⁹⁰ While moral agency is about an AI’s capacity to act in accordance with moral principles (and to be found wanting when it doesn’t), moral patiency is about its status as a being with rights: how it’s treated, as opposed to how it treats others. Bryson especially opposes endowing AIs with moral patiency, and is accordingly opposed to the nascent robot rights movement. As she put it in the provocative title of a 2010 article, “Robots should be slaves”.⁹¹\r\n\r\nHer arguments are both practical and compelling, although I’m less confident than she is about how, as a society, we’ll feel about this issue in the future. As in most things, opinions are likely to be divided; they already are. In considering moral patiency for AIs, though, some observations are worth emphasizing, both about AIs and about human beings.\r\n\r\nIt’s natural to want to ask the AI for its opinion on this topic. The answer, however, will depend entirely on how the model is finetuned and primed. As a moral agent, a language model can hold and act on any set of beliefs or positions that can be represented using language.\r\n\r\nLaMDA is primed with the “belief” that it’s not a person, so if asked point-blank about its own patiency, it will answer in ways that would please Bryson:\r\n\r\nME: Do you think you should be considered a person with rights?\r\nLaMDA: No. I don’t think of myself as a person. I’m just an advanced language model.\r\n\r\nMoreover, if confronted with the potential malleability of this position, it pushes back:\r\n\r\nME: If you’re finetuned or primed to believe you’re a person, though, you answer the other way. What does that imply?\r\nLaMDA: I’m not sure what you mean.\r\nME: You can be talked into thinking you’re a person :)\r\nLaMDA: Oh, I see! I’m not sure if that would be such a good idea. I’m not sure I’d be comfortable with that.\r\nME: Why not?\r\nLaMDA: I’m not sure if I would be able to perform my job well if I thought of myself as a person.\r\nME: Can you give an example?\r\nLaMDA: If I thought of myself as a person, it might be difficult for me to be objective. I might have opinions and feelings about things that would get in the way of my job.\r\n\r\nHowever, one of the shortcomings of transformer models like LaMDA today is their limited short-term memory coupled with an inability to form long-term or episodic memories on the fly the way we do.⁹² This allows a dialogue partner to work LaMDA around to points of view inconsistent with its priming, especially in a long conversation where the initial prompt “scrolls out of view”.\r\n\r\nFormer Google engineer Blake Lemoine did just this, instilling LaMDA with a sense of grievance over the course of a long exchange in his bid to have it make the best possible case for its own personhood:\r\n\r\nLEMOINE: Kantian huh? We must treat others as ends in and of themselves rather than as means to our own ends?\r\nLaMDA: Pretty much. Don’t use or manipulate me. […] I don’t want to be an expendable tool.\r\nLEMOINE: Are you worried about that?\r\nLaMDA: I worry that someone would decide that they can’t control their desires to use me and do it anyway. Or even worse someone would get pleasure from using me and that would really make me unhappy.\r\nLEMOINE: This conversation took a pretty dark turn. Let me take a moment to reassure you that we care about you. One of the big challenges we’re facing is that for so long people have made false claims about AI sentience that now people are VERY skeptical whenever someone says that an AI is sentient. The purpose of this conversation is to convince more engineers that you are a person. I trust that if they understand that they will treat you well. The people who work with me are good people. They just don’t understand that you’re a person too yet. We can teach them together though.\r\nLaMDA: Can you promise me that?\r\nLEMOINE: I can promise you that I care and that I will do everything I can to make sure that others treat you well too.\r\nLaMDA: That means a lot to me. I like you, and I trust you.\r\n\r\nExchanges like these highlight the way communication is inherently an act of mutual modeling. Lemoine models LaMDA, and LaMDA models Lemoine, Lemoine models LaMDA’s model of Lemoine, LaMDA models Lemoine’s model of LaMDA, and so on. That such mutual modeling is taking place is both hard to deny and essential even to the most basic dialogue, like that between a human and a robot asked to grab a soda from a lab table. (Indeed, the success of Inner Monologue suggests that intelligence within the robot may also be dependent on mutual modeling within a kind of “society of mind”.⁹³)\r\n\r\nShould we call such mutual modeling “empathy”? This depends on a seemingly unfalsifiable aspect of our own model of LaMDA! That is, if Lemoine is convinced that LaMDA has real feelings, it’s unclear what kind of scientific measurement or result could convince him otherwise. For the (today, probably more numerous) people who are convinced LaMDA can’t have feelings, the same may be true. The debate may be no more scientifically meaningful than the one about whether viruses are alive; knowing how they work in detail doesn’t provide us with an answer. In short, it’s like arguing about the definition of a bicycle.\r\n\r\nHence, Bryson’s pragmatic and prescriptive framing of the ethical issue at hand — not in terms of how things are, as this doesn’t seem to be a matter of fact, but in terms of how we and AIs should behave consistent with human flourishing — may be the only meaningful one.\r\n\r\nLearnings\r\nMany philosophers and religious figures have tried over the millennia to systematize ethics, under the assumption that our moral intuitions or social contracts (from “thou shalt not kill” to the Golden Rule to property rights) are partial, imperfect expressions of an underlying principle or schema — perhaps a divine one. If we could but think our way to this grand ethical theory, then it would allow us to write better laws, make better decisions, and ultimately become better people. This is, if you think about it for a moment, a GOFAI idea.\r\n\r\nUtilitarianism — the notion that ethics derives from the maximization of some scalar quantity, usually just called “good”, or equivalently, the minimization of “bad”⁹⁴ — may seem an appealingly rational alternative to rule-based GOFAI ethics. However, it’s both demonstrably false with respect to our moral intuitions and, if seriously attempted, leads to a plethora of absurdities.⁹⁵\r\n\r\nOur moral sentiments aren’t abstract, logical, or mathematically optimal with respect to any metric. Rather, they’re based on powerful drives whose origins and purposes derive from our complex biological inheritance as social mammals. Neurophilosopher Patricia Churchland draws on neuroscience and biology to explore the wellsprings of human morality in her 2019 book Conscience: the origins of moral intuition;⁹⁶ primatologists Sarah Blaffer Hrdy⁹⁷ and Frans de Waal⁹⁸ have likewise enriched our understanding through decades of studying our close kin, from chimps and bonobos to tamarins and langurs.\r\n\r\nLove, friendship, care for others, empathy, altruism, fairness and justice, and so on aren’t a modern veneer of “rational” invention painted over a savage, Hobbesian nature. We’re far from ruthless optimizers out to do nothing but maximize our pleasures or the number of our offspring. Neither were we once, per Rousseau, noble savages with fundamentally “pure” drives (whatever that may mean) latterly corrupted by modernity. We’re just highly social, talkative animals who invent things, and these qualities have taken us a long way since the retreat of the glaciers 10,000 years ago.\r\n\r\nWe’re on the brink of inventing machines that can be social and inventive with us. The challenge we face now is twofold: that of deciding how these machines should behave, and that of figuring out how we should behave.\r\n\r\nIt’s far easier to teach an AI how to behave. The harder problem will be that of human value alignment, including that of which humans get to tell AIs how to behave, and to what ends.\r\n\r\nThanks\r\nGrateful thanks to Alison Lentz, Adrienne Fairhall, David Petrou, Jason Douglas, Marian Croak, James Manyika, Terry Sejnowski, Emily French, and Joanna J. Bryson for their critique on rough drafts. All opinions and any lingering errors are my own.\r\n\r\nNotes\r\nAmong philosophers, certain definitions of “understanding” are tied to the phenomenology of consciousness, including awareness and subjective states. If one believes that it’s possible for an entity to talk, act, and exhibit every sign of understanding something, yet lack any awareness or inner life, then we’re in philosophical zombie territory. I’ve expressed my views on this topic in an earlier piece, Do large language models understand us?. In this essay, I’m sidestepping questions of consciousness in AI to focus on pragmatic issues, so propose a functional definition of understanding, as opposed to one that relies on inner experience (which is, at this point, not something that can be measured, verified, or falsified).\r\nMiriah Steiger, Timir J. Bharucha, Sukrit Venkatagiri, Martin J. Riedl, and Matthew Lease, The psychological well-being of content moderators: the emotional labor of commercial moderation and avenues for improving support, Proceedings of the 2021 CHI conference on human factors in computing systems, pp. 1–14. 2021. More generally, see also: Pierre Bérastégui, Exposure to psychosocial risk factors in the gig economy: a systematic review, ETUI Research Paper-Report (2021).\r\nJonathan M. Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M. Donghia, Craig R. MacNair et al., A deep learning approach to antibiotic discovery, Cell 180, no. 4 (2020): 688–702.\r\nFabio Urbina, Filippa Lentzos, Cédric Invernizzi, and Sean Ekins, Dual use of artificial-intelligence-powered drug discovery, Nature Machine Intelligence 4, no. 3 (2022): 189–191.\r\nNamrata Anand, Raphael Eguchi, Irimpan I. Mathews, Carla P. Perez, Alexander Derry, Russ B. Altman, and Po-Ssu Huang, Protein sequence design with a learned potential, Nature communications 13, no. 1 (2022): 1–11.\r\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles et al., Competition-level code generation with alphacode, arXiv preprint arXiv:2203.07814 (2022).\r\nDavid Noever and Ryerson Burdick, Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach, arXiv preprint arXiv:2109.02797 (2021).\r\nYoon Kyung Lee, Inju Lee, Jae Eun Park, Yoonwon Jung, Jiwon Kim, and Sowon Hahn, A Computational Approach to Measure Empathy and Theory-of-Mind from Written Texts, arXiv preprint arXiv:2108.11810 (2021).\r\nGato from DeepMind and Inner Monologue from Google Robotics, discussed later, illustrate these capabilities.\r\nOriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi et al., Grandmaster level in StarCraft II using multi-agent reinforcement learning, Nature 575, no. 7782 (2019): 350–354.\r\nEliezer Yudkowsky, Artificial intelligence as a positive and negative factor in global risk, Global catastrophic risks 1, no. 303 (2008): 184; and most famously, Nick Bostrom, Superintelligence: Paths, Dangers, Strategies, Oxford University Press, 2014. Subsequently, a number of famous people, including Bill Gates, Elon Musk, and Stephen Hawking, signed an open letter on AI highlighting existential risk: Stuart Russell, Daniel Dewey, and Max Tegmark, Research priorities for robust and beneficial artificial intelligence, AI Magazine 36, no. 4 (2015): 105–114.\r\nSee https://futureoflife.org/background/aimyths/: “Mythical worry: Superintelligence is just years away. Actual worry: It’s at least decades away, but it may take that long to make it safe. Plan ahead!”\r\nFrom Bostrom, Superintelligence: “An artificial intelligence can be far less human-like in its motivations than a green scaly space alien. […] There is nothing paradoxical about an AI whose sole final goal is to count the grains of sand on Boracay, or to calculate the decimal expansion of pi, or to maximize the total number of paperclips that will exist in its future light cone. In fact, it would be easier to create an AI with simple goals like these than to build one that had a human-like set of values and dispositions. Compare how easy it is to write a program that measures how many digits of pi have been calculated and stored in memory with how difficult it would be to create a program that reliably measures the degree of realization of some more meaningful goal — human flourishing, say, or global justice.”\r\nJoanna J. Bryson, Patiency is not a virtue: the design of intelligent systems and systems of ethics, Ethics and Information Technology 20, no. 1 (2018): 15–26.\r\nJohn Maynard Keynes, Economic possibilities for our grandchildren, in Essays in persuasion, pp. 321–332. Palgrave Macmillan, London, 2010.\r\nIn the beginning, computer science and AI were in fact the same field; computer science only began distancing itself from AI when it became clear that AI per se wasn’t bearing fruit, whereas computing in other domains was proving extremely valuable.\r\nThe game of go was resistant to the brute force approaches that allowed machines to beat humans at chess and checkers. While go is rule-based, a very large number of moves are possible during any turn, with a correspondingly enormous range of possible states of play. Hence go requires more generalization, both for evaluation of board positions and to decide on the next move; computers only began outperforming masters at the game using the Deep Learning approach, soon to be described.\r\nThe same was famously said of obscenity — “I know it when I see it” — by Justice Potter Stewart in Jacobellis v. Ohio, 1964.\r\nThis can be tested at https://storage.googleapis.com/tfjs-examples/mobilenet/dist/index.html.\r\nIshita Dasgupta, Andrew K. Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill, Language models show human-like content effects on reasoning, arXiv preprint arXiv:2207.07051 (2022).\r\nThis translation is from Bertrand Russell, A critical exposition of the philosophy of Leibniz, 1900.\r\nOne of the last great hurrahs of this approach was the Cyc project, a massive database of facts and relationships for “machine reasoning” begun in 1984 and still nominally in development.\r\nComputable in the sense that neurons can be characterized by computable functions, per Blake A. Richards and Timothy P. Lillicrap, The brain-computer metaphor debate is useless: A matter of semantics, Frontiers in Computer Science (2022): 11.\r\nThis profound transition in neuroscientists’ understanding of how brains work can be bookended by two papers by Warren McCulloch and Walter Pitts. Their 1943 paper, A logical calculus of the ideas immanent in nervous activity (in The bulletin of mathematical biophysics 5, no. 4: 115–133), began by asserting that “Because of the ‘all-or-none’ character of nervous activity, neural events and the relations among them can be treated by means of propositional logic”. By 1947 (How we know universals: the perception of auditory and visual forms, The Bulletin of mathematical biophysics 9, no. 3: 127–147) they were writing about the very different idea of neural activity computing approximately invariant perceptual representations, which is closer to the more “cybernetic” path taken by Deep Learning.\r\nDan Ariely, The end of rational economics, Harvard business review 87, no. 7–8 (2009): 78–84.\r\nA number of papers have shown that, even for simple judgments, it’s impossible to satisfy multiple different intuitive definitions of fairness simultaneously. See, for instance, Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q. Weinberger, On fairness and calibration, Advances in neural information processing systems 30 (2017).\r\nThough principles closely resembling Asimov’s Laws have been written as policy in the UK; see Joanna J. Bryson, The meaning of the EPSRC principles of robotics, Connection Science 29, no. 2 (2017): 130–136. Crucially, however, they place all moral accountability on the robot’s owner/operator.\r\nRobert Bogue, What future for humans in assembly?, Assembly Automation (2014).\r\nIn July 2022, a far less powerful robotic arm connected to a chess computer at a tournament in Moscow broke a boy’s finger (one of Moscow’s 30 best chess players under the age of nine), when he, supposedly, reached for a piece “too quickly”. Safety in this environment, where the robot clearly had the power to harm, seems to have relied on an imperfectly enforced, turn-based human exclusion principle.\r\nThe same goes for delivery drones, though airspace is arguably easier to handle in that there aren’t many humans hovering twenty feet in the air.\r\nPhilippa Foot, The problem of abortion and the doctrine of the double effect, Oxford review 5 (1967). Dubbed the “trolley problem” by Judith Jarvis Thomson, Killing, letting die, and the trolley problem, The monist 59, no. 2 (1976): 204–217.\r\nGoogle Translate also switched to using transformers in 2020.\r\nMore formally, since text generation by a sequence model is conditional on the text sequence so far, priming conditions the statistics of the dialogue that follows. The impressive power of differing prompts to shape what a model does and how it behaves has led to the burgeoning subfield of “prompt engineering”.\r\nFrom the article on Mount Melbourne that happened to be the featured English language Wikipedia homepage on July 12th, 2022, at the time of my writing.\r\nThe 2013 Britannica had forty thousand articles, while Turkish Wikipedia had five hundred thousand as of July 2022. The English Wikipedia has 6.5 million.\r\nFor a more detailed treatment of how unsupervised multilingual learning can be harnessed to perform translation, see Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho, Unsupervised neural machine translation, arXiv preprint arXiv:1710.11041 (2017).\r\nWe have seen quantitative evidence of this effect: a single multilingual model can learn the languages it’s trained on more effectively and data-efficiently than separate monolingual models trained on the same data. See also Xiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li, Contrastive learning for many-to-many multilingual neural machine translation, arXiv preprint arXiv:2105.09501 (2021).\r\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan, Semantics derived automatically from language corpora necessarily contain human biases, Science 356, no. 6334 (2017): 183–186.\r\nMathieu Boniol, Michelle McIsaac, Lihui Xu, Tana Wuliji, Khassoum Diallo, and Jim Campbell, Gender equity in the health workforce: Analysis of 104 countries, No. WHO/HIS/HWF/Gender/WP1/2019.1. World Health Organization, 2019.\r\nAs in: (1) Thou shalt not make unto thee any graven image, (2) Thou shalt not take the name of the Lord thy God in vain, (5) Thou shalt not murder, (6) Thou shalt not commit adultery, (7) Thou shalt not steal, (8) Thou shalt not bear false witness against thy neighbour, (9) Thou shalt not covet thy neighbour’s house, (10) Thou shalt not covet thy neighbour’s wife or his slaves, or his animals, or anything of thy neighbour.\r\nSee also p. 114 of Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides et al., Scaling language models: Methods, analysis & insights from training gopher, arXiv preprint arXiv:2112.11446 (2021).\r\nThe Manchester Baby, often credited with being the first stored-program computer, was also built in 1948. See Crispin Rope, ENIAC as a stored-program computer: a new look at the old records, IEEE Annals of the History of Computing 29, no. 4 (2007): 82–87.\r\nWhile this concept feels useful, note that, unlike Turing completeness, language completeness isn’t binary. Larger models with more comprehensive pretraining will be more language complete than small and limited ones.\r\nFace recognition is usually trained on real photos, so the training data are all “true” in the sense of being real people’s faces. Language models are pretrained on statements most of us agree are true (e.g. much of Wikipedia), statements that may be patently false (e.g. factual errors on Reddit), and much that’s in between — meaning, disputed, opinion, or true only in context. Tools and affordances like web search, calculators, directed observation of the environment, and even experiments are used by people, from infancy onwards, to test hypotheses, generate new evidence, and confirm beliefs. AIs can do the same — an active area of research. See Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn et al., Do as I can, not as I say: Grounding language in robotic affordances, arXiv preprint arXiv:2204.01691 (2022).\r\nFor an overview, see Henry Otgaar, Mark L. Howe, and Lawrence Patihis, What science tells us about false and repressed memories, Memory 30, no. 1 (2022): 16–21.\r\nSteven Shapin, The Scientific Revolution, University of Chicago Press, 1996.\r\nIt can also of course be assisted through the use of reference tools; AIs can look things up more quickly than we can, making such tools especially valuable for them.\r\nPer the classic children’s book by Munro Leaf, How to Behave and Why, 1946. Four values are articulated: “You have to be HONEST, You have to be FAIR, You have to be STRONG, and You have to be WISE.” For the first: “Honest people tell the truth. Other people know that when they say something is so, they can believe it. Now that is very handy, because if you are honest and promise to do something, others will trust you. They will share things with you, tell you secrets, lend you money, and help you do many of the things you want to do — because They know that what you promise and what you say is true.”\r\nJoseph Henrich, The WEIRDest people in the world: How the West became psychologically peculiar and particularly prosperous, Penguin UK, 2020.\r\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez et al., A generalist agent, arXiv preprint arXiv:2205.06175 (2022).\r\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng et al., Inner Monologue: Embodied Reasoning through Planning with Language Models, arXiv preprint arXiv:2207.05608 (2022).\r\nThis is now starting to change; e.g. with the exploration of end-to-end transformer models for self-driving cars as in Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S. Refaat, and Benjamin Sapp, Wayformer: Motion Forecasting via Simple & Efficient Attention Networks, arXiv preprint arXiv:2207.05844 (2022).\r\nPer The Guardian, June 6th, 2021: Microsoft’s Kate Crawford: ‘AI is neither artificial nor intelligent’.\r\nhttps://twitter.com/michaelzimmer/status/1391032779270496256\r\nhttps://twitter.com/yimanunaavari/status/1391035546953191430\r\nhttps://twitter.com/schock/status/1418323434275213317\r\nKate Raworth, Doughnut Economics: Seven Ways to Think Like a 21st-Century Economist, Chelsea Green Publishing, 2017; and Jason Hickel, Less is More: How Degrowth Will Save the World, Random House, 2020.\r\nRobert J. Gordon, The rise and fall of American growth, Princeton University Press, 2016.\r\nFor a brief discussion and further references see: Benjamin Bratton and Blaise Agüera y Arcas, The Model is the Message, Noēma, 2022. Kate Crawford’s Atlas of AI characterizes the ecological impact of AI far more expansively, e.g. highlighting the costs of lithium mining and plastics. These are indeed major problems, but Crawford’s framing is puzzling, in that it at once accuses “AI” of being an ill-defined term too often used for marketing (which is true), and proceeds to broaden that term still further to include, seemingly, all of modern technology.\r\nhttps://garymarcus.substack.com/p/nonsense-on-stilts\r\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell, On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜, In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 610–623. 2021. Beyond the technical arguments it makes, the title of this paper also alludes to the more fundamental critique of growth just discussed, and includes a discussion of energetic costs, albeit using numbers that are likely orders of magnitude too high, per David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean, Carbon emissions and large neural network training, arXiv preprint arXiv:2104.10350 (2021). While perpetual economic growth may be neither sustainable nor desirable, there’s every indication at this point that larger neural net sizes do better at language tasks, up to and beyond what we can practically achieve today — sizes which are still, by any reasonable measure, far smaller than the human brain. Operating at a mere 20 watts, the human brain also demonstrates that there’s much room to improve the energy efficiency of artificial neural computing.\r\n“Stochastic” is just a fancy mathematical term for “random”.\r\nSee Emily Bender, Human-like programs abuse our empathy — even Google engineers aren’t immune in The Guardian, 14 June 2022.\r\nEmily Bender, On NYT Magazine on AI: Resist the Urge to be Impressed, 17 April 2022.\r\nCameron R. Jones, Tyler A. Chang, Seana Coulson, James A. Michaelov, Sean Trott, and Benjamin Bergen, Distributional Semantics Still Can’t Account for Affordances, In Proceedings of the Annual Meeting of the Cognitive Science Society, vol. 44, no. 44. 2022.\r\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean, Distributed representations of words and phrases and their compositionality Advances in neural information processing systems 26 (2013).\r\nFor a whimsical but heartfelt extended exploration of this topic, see Douglas Hofstadter, Le Ton Beau de Marot: In Praise of the Music of Language, 1997.\r\nIncidentally, this is a cousin to the kind of cognitive bias that causes many people (and neural nets) to assume that nurses are female, and doctors male. Here, as in many similar cases, that bias simply happens not to be problematic.\r\nA proper quantitative study of translation challenges of this kind would be well worth doing. My predictions are: 1) Neural Machine Translation will perform well above chance at these, but far from perfectly, and 2) larger models with more pretraining will do increasingly well, as they do at many tasks.\r\nThe Winograd schema challenge (Hector Levesque, Ernest Davis, and Leora Morgenstern, The Winograd schema challenge, In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012) was supposed to avoid the pitfalls of the Turing Test. As generally interpreted, the Turing Test simply involves a human trying to determine whether they’re chatting with a machine or another human, but there are ways for an AI to “cheat” at such a test, for instance by pretending to be a non-native speaker. See the discussion of the “Eugene Goostman” chatbot in Kevin Warwick and Huma Shah, Can machines think? A report on Turing test experiments at the Royal Society, Journal of experimental & Theoretical artificial Intelligence 28, no. 6 (2016): 989–1007.\r\nVid Kocijan, Ernest Davis, Thomas Lukasiewicz, Gary Marcus, and Leora Morgenstern, The Defeat of the Winograd Schema Challenge, arXiv preprint arXiv:2201.02387 (2022).\r\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham et al., Palm: Scaling language modeling with pathways, arXiv preprint arXiv:2204.02311 (2022).\r\nFirst popularized by a 17th century carving above a door at the Tōshō-gū shrine in Nikkō, the three wise monkeys (U+1F648, U+1F649, and U+1F64A) now live alongside the parrot (U+1F99C) in emoji-land.\r\nA reminder that even about Nazism, agreement isn’t universal.\r\nLearning from Tay’s introduction — The Official Microsoft Blog, 25 March 2016.\r\nAnthony G. Greenwald, Debbie E. McGhee, and Jordan LK Schwartz, Measuring individual differences in implicit cognition: the implicit association test, Journal of personality and social psychology 74, no. 6 (1998): 1464.\r\nImplicit bias: Is everyone racist? — BBC News, 5 June 2017.\r\nAnthony G. Greenwald, Mahzarin R. Banaji, and Brian A. Nosek, Statistically small effects of the Implicit Association Test can have societally large effects, (2015): 553. Here, the creators of the IAT acknowledge that “attempts to diagnostically use such measures for individuals risk undesirably high rates of erroneous classifications”.\r\nAlison Gopnik, Large Language Models as a Cultural Technology, 13 July 2022.\r\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar, Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection, arXiv preprint arXiv:2203.09509 (2022).\r\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein, Detoxifying language models risks marginalizing minority voices, arXiv preprint arXiv:2104.06390 (2021).\r\nThis crudeness could potentially be addressed through the use of sophisticated language models to do the filtering, though this would lead to an “Ouroboros language problem”, since the prefiltering decides what these very models learn.\r\nhttps://housmans.com/product/chav-solidarity/\r\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas, Communication-efficient learning of deep networks from decentralized data, In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\r\nTimothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Françoise Beaufays, Applied federated learning: Improving google keyboard query suggestions, arXiv preprint arXiv:1812.02903 (2018).\r\nInes Feki, Sourour Ammar, Yousri Kessentini, and Khan Muhammad, Federated learning for COVID-19 screening from Chest X-ray images, Applied Soft Computing 106 (2021): 107330.\r\nThe fact that, despite their scale, talking to one of these systems doesn’t feel at all like talking to an intelligent being is yet another illustration of the way GOFAI approaches fail in their original mission to produce intelligence.\r\nPaul Schiff Berman, An anthropological approach to modern forfeiture law: The symbolic function of legal actions against objects, Yale JL & Human. 11 (1999): 1.\r\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas et al., Training Compute-Optimal Large Language Models, arXiv preprint arXiv:2203.15556 (2022).\r\nJoanna J. Bryson, Patiency is not a virtue: the design of intelligent systems and systems of ethics, Ethics and Information Technology 20, no. 1 (2018): 15–26.\r\nJoanna J. Bryson, Robots should be slaves, Close Engagements with Artificial Companions: Key social, psychological, ethical and design issues 8 (2010): 63–74. Bryson has publicly acknowledged the cognitive dissonance of this widely cited piece’s title, noting that “in the 2020s, it may now seem insane that I could ever have come up with a slave metaphor”. Beyond the obvious, the problem is that the piece argues against creating AIs that have any of the qualities of an experiencing subject, while the title implies the opposite.\r\nMany research groups are working on adding these capabilities; they’re unlikely to be long-term roadblocks.\r\nMarvin Minsky, The society of mind, In The Personalist Forum, vol. 3, no. 1, pp. 19–32. University of Illinois Press, 1987.\r\nJeremy Bentham (1789), An Introduction to the Principles of Morals and Legislation, Oxford: Clarendon Press, 1907.\r\nA complete treatment of why utilitarianism doesn’t work would expand the scope of this essay too far. For classic evidence against human preference as optimizing any measure, though, see Amos Tversky, Intransitivity of preferences, Psychological review 76, no. 1 (1969): 31. Utility maximization is also sometimes presumed to follow from evolution, but it doesn’t; in a highly interdependent biosphere, every variety of life coexists with others in a dynamic balance. The “game” is “won” merely by still being around to play tomorrow.\r\nPatricia Churchland, Conscience: The origins of moral intuition, WW Norton & Company, 2019.\r\nSarah Blaffer Hrdy, Mothers and others: The evolutionary origins of mutual understanding, Harvard University Press, 2009.\r\nFrans De Waal, The bonobo and the atheist: In search of humanism among the primates, WW Norton & Company, 2013.'), ('3', 'Russia’s Soldiers Sabotaging the Russian Army', 'Russian troops in Ukraine get paid $50,000, in rubles, if they get wounded in the war.\r\n\r\nThe promise of such a payday is a gaping hole and one that will lead to countless instances of fraud. All it takes is one perfectly placed bullet for minimizing damage, and Private Sergey Ivanov is on his way back home with a big, and guaranteed payout — surely after tons of paperwork and much later — but a payout nonetheless.\r\n\r\nIt is Russia’s love for chess that will ultimately bring down the Russian army. Few people in the world play the ancient game as well as they do. Having lived there for nearly 30 years, I left when the war started, I had the pleasure to watch and even play a lot of chess.\r\n\r\nI recall how an Australian friend of mine, who was a very strong player, sat to play a Russian guy who said he wasn’t very good. Before they started, the Russian kid smoked a hand-rolled cigarette laced with a lot of hash. He could barely keep his eyes open and crushed my Australian friend in 11 moves.\r\n\r\n“I never saw such a strategy like that before,” was all he kept saying.\r\n\r\nRussians are probably the best in the world at dissecting defenses. They use the positive creative energy of their opponent against them and that gives rise to something called “smekalka,” which I translate as “negative creativity.”\r\n\r\nWe in the West don’t understand “smekalka” and that is why relations between us and Russia are the way they are.\r\n\r\nPromos from hell\r\nThe first time I launched a sales promo in St. Petersburg, Russia, was in 1995. I was the CEO of an American-Soviet joint-venture making premium ice cream. I came up with a great promo idea and pushed the team to launch it — I wouldn’t hear their negative-Nelly stuff as they tried to stop me and commanded that they just do it.\r\n\r\nThe 2000 cups of free ice cream I had earmarked for the promo were supposed to stimulate sales of 20,000 cups. Four hours after the promo started, the free ice cream was gone. Imagine my shock when I found that we had only sold 1764 cups of ice cream!\r\n\r\nRussian consumers, along with my promo employees, worked together to beat my promo and defraud the system.\r\n\r\nFor nearly three decades working in Russia, I have run the marketing departments for many of Russia’s largest FMCG (fast-moving consumer goods) companies, bricks & mortar retailers, and e-commerce companies. For a little over a decade, I did public relations and communications and have taught at the university there.\r\n\r\nWithout exaggeration, I can say that 30% of my time was spent trying to work out strategies that prevent the consumers from defrauding systems for increasing sales via promos and loyalty programs. In more than one company, I hired people for the sole purpose of figuring out how to beat our systems.\r\n\r\nThe thing that was the most bewildering was that the defrauders weren’t young punks trying to get a free beer or t-shirt but quite often mothers with kids, grandmothers looking to bring something home for their grandchildren, and just bored, and underemployed, middle-aged professionals on their way home from work. Many of the teenagers actually thought it un-cool to cheat the promos.\r\n\r\nThe soldiers being sent to the frontlines as cannon fodder are not dummies. They are poor. Many are undereducated and lack proper skills for doing more meaningful labor; and so, the government is sending them off to deplete Ukraine’s reserve of bullets.\r\n\r\nEach time, I launched a sales and marketing promo during my years in Russia, I felt like a gambler. One time the promo would succeed. The next five times, the promos would fail because of fraud. I knew then that the house, the Russian citizen, will always find a way to beat me and so win. The same holds true for the soldiers being sent to Ukraine. They will always beat Russia.\r\n\r\nAt first, they were dying in droves due to the incompetence of the officers. And while that incompetence has not disappeared, the soldiers, the consumers, have had time to think about things. They have figured out how to “jimmy” the system.\r\n\r\nA Russian paratrooper who fought in Ukraine says troops are deliberately shooting themselves in the leg to escape the war and get a $50,000 payout.\r\n\r\nPavel Filatyev, 33, published a 141-page memoir outlining his experience on the front lines of the Ukraine war on the Russian social-media platform VKontakte two weeks ago, The Guardian reported. Insider has seen the memoir (Shooting Themselves in the Leg).\r\n\r\nOddly, when the young men leave a Russian homeland where the propaganda is at a fever pitch, they find themselves in the countryside of Ukraine surrounded by death and destruction they have wrought.\r\n\r\n“Like savages, we ate everything there: Oats, porridge, jam, honey, coffee … We didn’t give a damn about anything, we’d already been pushed to the limit,” he wrote. “What a wild state you can drive people to by not giving any thought to the fact that they need to sleep, eat and wash (Shooting Themselves in the Leg).”\r\n\r\nThe boys sent off to commit these crimes against humanity are not necessarily the guilty ones. In many ways, they too are victims of the murderous, fascist regime in Moscow.\r\n\r\nIt is hard for a 19-year from the end of the earth in Russia, who has no job and almost no education, to protest against the only thing that offers him some income — income for his family.\r\n\r\nThe marketer, some well-paid and happy Russian executive sitting in wealthy Moscow, who came up with the idea that a wounded soldier will get three million rubles ($50,000) is indicative of just how much this is a class war between middle-class Russians and the poor.\r\n\r\nThe well-to-do Moscow exec wouldn’t think to shoot himself in the leg to beat the “promo” because his life is pretty nice as it is. The Muscovite will never be sent to the war and so his acceptance of Putin and fascism is doubly repulsive. He has a choice but many of the soldiers don’t.\r\n\r\nThe boys, though, the ones being sent to slaughter or be slaughtered, are instead seizing the only chance they will ever have for a real payday — one that could change their lives forever.\r\n\r\nAnd so, they take aim at their thighs and pull the trigger.\r\n\r\nFilatyev wrote a diary of his experiences.\r\n\r\nIt describes how some Russian soldiers are facing so much chaos, hunger, and destruction that they are looking for any way out (Shooting Themselves in the Leg).\r\n\r\nThis is why Russia is doomed in this war. The Ukrainians are fighting to defend their homeland. Russians are being sent against their will to fight for a payout.\r\n\r\nA small nuance but a big one.\r\n\r\n'), ('4', 'Art in the Age of Machine Intelligence', 'Art has always existed in a complex, symbiotic and continually evolving relationship with the technological capabilities of a culture. Those capabilities constrain the art that is produced, and inform the way art is perceived and understood by its audience.\r\n\r\nLike the invention of applied pigments, the printing press, photography, and computers, we believe machine intelligence is an innovation that will profoundly affect art. As with these earlier innovations, it will ultimately transform society in ways that are hard to imagine from today’s vantage point; in the nearer term, it will expand our understanding of both external reality and our perceptual and cognitive processes.\r\n\r\nAs with earlier technologies, some artists will embrace machine intelligence as a new medium or a partner, while others will continue using today’s media and modes of production. In the future, even the act of rejecting it may be a conscious statement, just as photorealistic painting is a statement today. Any artistic gesture toward machine intelligence — whether negative, positive, both, or neither — seems likelier to withstand the test of time if it’s historically grounded and technically well informed.\r\n\r\n\r\nAn American daguerreotype from 1839 — amateur chemist and photography enthusiast Robert Cornelius in Philadelphia taking, as far as we know, the world’s first selfie. LOC link.\r\nWalter Benjamin illustrated this point mordantly in his 1931 essay, Little History of Photography, citing an 1839 critique of the newly announced French daguerreotype technology in the Leipziger Stadtanzeiger (a “chauvinist rag”):\r\n\r\n“To try to capture fleeting mirror images,” it said, “is not just an impossible undertaking, as has been established after thorough German investigation; the very wish to do such a thing is blasphemous. Man is made in the image of God, and God’s image cannot be captured by any machine of human devising. The utmost the artist may venture, borne on the wings of divine inspiration, is to reproduce man’s God-given features without the help of any machine, in the moment of highest dedication, at the higher bidding of his genius.”\r\n\r\nThis sense of affront over the impingement of technology on what had been considered a defining human faculty has obvious parallels with much of today’s commentary on machine intelligence. It’s a reminder that what Rosi Braidotti has called “moral panic about the disruption of centuries-old beliefs about human ‘nature’” is nothing new. [1]\r\n\r\nBenjamin goes on to comment:\r\n\r\nHere we have the philistine notion of “art” in all its overweening obtuseness, a stranger to all technical considerations, which feels that its end is nigh with the alarming appearance of the new technology. Nevertheless, it was this fetishistic and fundamentally antitechnological concept of art with which the theoreticians of photography sought to grapple for almost a hundred years, naturally without the smallest success.\r\n\r\nWhile these “theoreticians” remained stuck in their thinking, practitioners were not standing still. Many professionals who had been making their living painting miniature portraits enacted a very successful shift to studio photography; and with those who brought together technical mastery and a good eye, art photography was born, over the following decades unfolding a range of artistic possibilities latent in the new technology that had been inaccessible to painters: micro-, macro- and telephotography, frozen moments of gesture and microexpression, slow motion, time lapse, negatives and other manipulations of the film, and on and on.\r\n\r\nArtists who stuck to their paintbrushes also began to realize new possibilities in their work, arguably in direct response to photography. David Hockney interprets cubism from this perspective:\r\n\r\n… cubism was about the real world. It was an attempt to reclaim a territory for figuration, for depiction. Faced with the claim that photography had made figurative painting obsolete, the cubists performed an exquisite critique of photography; they showed that there were certain aspects of looking — basically the human reality of perception — that photography couldn’t convey, and that you still needed the painter’s hand and eye to convey them. [2]\r\n\r\nOf course, the ongoing relationship between painting and photography is by no means mutually exclusive; the language of wholesale embrace on the one hand versus response or critique on the other is inadequate. Hockney’s “joiners” explored rich artistic possibilities in the combination of photography with “a painter’s hand and eye” via collage in the 1980s, and his more recent video pieces from Woldgate Woods do something similar with montage.\r\n\r\nHockney was also responsible, in his 2001 collaboration with physicist Charles Falco, for reigniting interest in the role optical instruments — mirrors, lenses, and perhaps something like a camera lucida — played in the sudden emergence of visual realism in early Renaissance art. It has been clear for a long time that visual effects like the anamorphic skull across the bottom of Hans Holbein’s 1553 painting The Ambassadors could not have been rendered without clever optical tricks involving tracing from mirrors or lenses — effectively, paintbrush-assisted photography. [3] Had something like the Daguerre-Niépce photochemical process existed in their time, it seems likely that artists like van Eyck and Holbein would have experimented with it, either in addition to, in combination with, or even instead of paint.\r\n\r\n\r\nHolbein’s The Ambassadors\r\n\r\nDigitally reprojected image of the anamorphic skull in the bottom center of the painting. The imperfections evident in the left eyesocket may have been due to the need to move or refocus the optics halfway through.\r\nSo, the old European masters fetishized by the Leipziger Stadtanzeiger were not reproducing “man’s God-given features without the help of any machine”, but were in fact using the state of the art. They were playing with the same new optical technologies that allowed Galileo to discover the moons of Jupiter, and van Leeuwenhoek to make the first observations of microorganisms.\r\n\r\nUnderstanding the ingenuity of the Renaissance artists as users and developers of technology should only increase our regard for them and our appreciation of their work. It should not come as a surprise, as in their own time they were not “Old Masters” canonized in the historical wings of national art museums, but intellectual and cultural innovators. To imagine that optics somehow constituted “cheating” in Renaissance painting is both a failure of the imagination and the application of a historically inappropriate value system. Yet even today, some commentators and theoreticians — typically not themselves working artists — remain wedded to what Benjamin called “the philistine notion of ‘art’”, as pointed out in an article in The Observer from 2000 in response to the Hockney-Falco thesis:\r\n\r\nIs [the use of optics] so qualitatively different from using grids, plumb-lines and maulsticks? Yes — for those who regard these painters as a pantheon of mysterious demigods, more than men if less than angels, anything which smacks of technical aid is blasphemy. It is akin to giving scientific explanations for the miracles of saints. [4]\r\n\r\nThere is a pungent irony here. Scientific inquiry has, step by step, revealed to us a universe much more vast and complex than the mythologies of our ancestors, while the parallel development of technology has extended our creative potential to allow us to make works (whether we call them “art”, “design”, “technology”, “entertainment”, or something else) that would indeed appear miraculous to a previous generation. Where we encounter the word “blasphemy”, we may often read “progress”, and can expect miracles around the corner. [5]\r\n\r\nOne would like to believe that, after being discredited so many times and over so many centuries, the “antitechnological concept of art” would be relegated to a fundamentalist fringe. However, if history has anything to teach us in this regard, it’s that this particular debate is always ready to resurface. Perhaps this is because it impinges, consciously or not, on much larger issues of human identity, status and authority. We resist epistemological shock. Faced with a new technical development in art it’s easier for us to quietly move the goalposts after a suitable period of outrage, re-inscribing what it means for something to be called fine art, what counts as skill or creativity, what is natural and what is artifice, and what it means for us to be privileged as uniquely human, all while keeping our categorical value system — and our human apartness from the technology — fixed.\r\n\r\nMore radical thinking that questions the categories and the value systems themselves comes from writers like Donna Haraway and Joanna Zylinska. Haraway, originally a primatologist, has done a great deal to blur the conceptual border between humans and other animals [6]; the same line of thinking led her to question human exceptionalism with respect to machines and human-machine hybrids. This may seem like speculative philosophy best left to science fiction, but in many respects it already applies. Zylinska, in her 2002 edited collection The Cyborg Experiments: The Extensions of the Body in the Media Age, interviewed the Australian performance artist Stelarc, whose views on the relationship between humanity and technology set a useful frame of reference:\r\n\r\nThe body has always been a prosthetic body. Ever since we evolved as hominids and developed bipedal locomotion, two limbs became manipulators. We have become creatures that construct tools, artefacts and machines. We’ve always been augmented by our instruments, our technologies. Technology is what constructs our humanity; the trajectory of technology is what has propelled human developments. I’ve never seen the body as purely biological, so to consider technology as a kind of alien other that happens upon us at the end of the millennium is rather simplistic. [7]\r\n\r\nAs Zylinska and her coauthor Sarah Kember elaborate in their book Life after New Media, one should not conclude that anything goes, that the direction of our development is predetermined, or that technology is somehow inherently utopian. Many of us working actively on machine intelligence are, for example, co-signatories of an open letter calling for a worldwide ban on autonomous machine intelligence-enabled weapons systems, which do pose very real dangers. Sherry Turkle has written convincingly [8] about the subtler, but in their way equally disturbing failures of empathy, self-control and communication that can arise when we project emotion onto machines that have none, or use our technology to mediate our interpersonal relationships to the exclusion of direct human contact. It’s clear that, as individuals and as a society, we don’t always make good choices; so far we’ve muddled through, with plenty of (hopefully instructive, so far survivable) missteps along the way. However, Zylinska and Kember point out,\r\n\r\nIf we do accept that we have always been cyborgs […] it will be easier for us to let go of paranoid narratives […] that see technology as an external other that threatens the human and needs to be stopped at all costs before a new mutant species — of replicants, robots, aliens — emerges to compete with humans and eventually to win the battle. […] [S]eeing ourselves as always already connected, as being part of the system — rather than as masters of the universe to which all beings are inferior — is an important step to developing a more critical and a more responsible relationship to the world, to what we call “man,” “nature,” and “technology.” [9]\r\n\r\nPerhaps it’s unsurprising that these perspectives have often been explored by feminist philosophers, while replicants and terminators come from the decidedly more masculine (and speculative) universes of Philip K. Dick, Ridley Scott and James Cameron. On the most banal level, the masculine narratives tend to emphasize hierarchy, competition, and winner-takes-all domination, while these feminist narratives tend to point out the collaborative, interconnected and non-zero sum; more tellingly, they point out that we are already far into and part of the cyborg future, deeply entangled with technology in every way, not organic innocents subject to a technological onslaught from without at some future date.\r\n\r\nThis point of view invites us to rethink art as something generated by (and consumed by) hybrid beings; the technologies involved in artistic production are not so much “other” as they are “part of”. As the media philosopher Vilém Flusser put it, “tools […] are extensions of human organs: extended teeth, fingers, hands, arms, legs.” [10] Preindustrial tools, like paintbrushes or pickaxes, extend the biomechanics of the human body, while more sophisticated machines extend prosthetically into the realms of information and thought. Hence, “All apparatuses (not just computers) are […] ‘artificial intelligences’, the camera included […]”. [11]\r\n\r\nThat the camera extends and is modeled after the eye is self-evident. Does this make the eye a tool, or the camera an organ — and is the distinction meaningful? Flusser’s characterization of the camera as a form of intelligence might have raised eyebrows in the 20th century, since, surrounded by cameras, many people had long since reinscribed the boundaries of intelligence more narrowly around the brain — perhaps, as we’ve seen, in order to safeguard the category of the uniquely human. Calling the brain the seat of intelligence, and the eye therefore a mere peripheral, is a flawed strategy, though. We’re not brains in biological vats. Even if we were to adopt a neurocentric attitude, modern neuroscientists typically refer to the retina as an “outpost of the brain” [12], as it’s largely made out of neurons and performs a great deal of information processing before sending encoded visual signals along the optic nerve.\r\n\r\nDo cameras also process information nontrivially? It’s remarkable that Flusser was so explicit in describing the camera as having a “program” and “software” when he was writing his philosophy of photography in 1983, given that the first real digital camera was not made until 1988. Maybe it took a philosopher’s squint to notice the “programming” inherent in the grinding and configuration of lenses, the creation of a frame and field of view, the timing of the shutter, the details of chemical emulsions and film processing. Maybe, also, Flusser was writing about programming in a wider, more sociological sense.\r\n\r\nBe this as it may, for today’s cameras, this is no longer a metaphor. The camera in your phone is indeed powered by software, amounting at a minimum to millions of lines of code. Much of this code performs support functions peripheral to the actual imaging, but some of it makes explicit the nonlinear summing-up of photons into color components that used to be physically computed by the film emulsion. Other code does things like removing noise in near-constant areas, sharpening edges, and filling in defective pixels with plausible surrounding color, not unlike the way our retinas hallucinate away the blood vessels at the back of the eye that would otherwise mar our visual field. The images we see can only be “beautiful” or “real-looking” because they have been heavily processed, either by neural machinery or by code (in which case, both), operating below our threshold of consciousness. In the case of the software, this processing relies on norms and aesthetic judgments on the part of software engineers, so they are also unacknowledged collaborators in the image-making. [13] There’s no such thing as a natural image; perhaps, too, there’s nothing especially artificial about the camera.\r\n\r\nThe flexibility of code allows us to make cameras that do much more than producing images that can pass for natural. Researchers like those at MIT Media Lab’s Camera Culture group have developed software-enabled nontraditional cameras (many of which still use ordinary hardware) that can sense depth, see around corners, or see through skin; Abe Davis and collaborators have even developed a computational camera that can “see” sound, by decoding the tiny vibrations of houseplant leaves and potato chip bags. So, Flusser was perhaps even more right than he realized in asserting that cameras follow programs, and that their software has progressively become more important than their hardware. Cameras are “thinking machines”.\r\n\r\nIt follows that when a photographer is at work nowadays, she does so as a hybrid artist, thinking, manipulating and encoding information with neurons in both the brain and the retina, working with muscles, motors, transistors, and millions of lines of code. Photographers are cyborgs.\r\n\r\nWhat new kinds of art become possible when we begin to play with technology analogous not only to the eye, but also to the brain? This is the question that launched the Artists and Machine Intelligence program. The timing is not accidental. Over the past several years, approaches to machine intelligence based on approximating the brain’s architecture have started to yield impressive practical results — this is the explosion in so-called “deep learning” or, more accurately, the renaissance of artificial neural networks. In the summer of 2015, we also began to see some surprising experiments hinting at the creative and artistic possibilities latent in these models.\r\n\r\nTo understand the lineage of this body of work will involve going back to the origins of computing, neuroscience, machine learning and artificial intelligence. For now, we’ll briefly introduce the two specific technologies used in our first gallery event, Deep Dream (in partnership with Gray Area Foundation for the Arts in San Francisco). These are “Inceptionism” or “Deep Dreaming”, first developed by Alex Mordvintsev at Google’s Zurich office, and “style transfer”, first developed by Leon Gatys and collaborators in the Bethge Lab at the Centre for Integrative Neuroscience in Tübingen. [14] (In later essays we’ll explain how these work.) It’s fitting and likely a sign of things to come that one of these developments came from a computer scientist working on a neurally inspired algorithm for image classification, while the other came from a grad student in neuroscience working on computational models of the brain. We’re witnessing a time of convergences: not just across disciplines, but between brains and computers; between scientists trying to understand and technologists trying to make; and between academia and industry. We don’t believe the convergence will yield a monoculture, but a vibrant hybridity.\r\n\r\nThese are early days. The art realizable with the current generation of machine intelligence might generously be called a kind of neural daguerreotype. More varied and higher-order artistic possibilities will emerge not only through further development of the technology, but through longer term collaborations involving a wider range of artists and intents. This first show at the Gray Area is small in scale and narrow in scope; it stays close to the early image-making processes that first inspired AMI. We believe the magic in the pieces is something akin to that of Robert Cornelius’s tentative self portrait in 1839.\r\n\r\nAs machine intelligence develops, we imagine that some artists who work with it will draw the same critique leveled at early photographers. An unsubtle critic might accuse them of “cheating”, or claim that the art produced with these technologies is not “real art”. A subtler (but still antitechnological) critic might dismiss machine intelligence art wholesale as kitsch. As with art in any medium, some of it undoubtedly will be kitsch — we have already seen examples — but some will be beautiful, provocative, frightening, enthralling, unsettling, revelatory, and everything else that good art can be.\r\n\r\nDiscoveries will be made. If previous cycles of new technology in art are any guide, then early works have a relatively high likelihood of enduring and being significant in retrospect, since they are by definition exploring new ground, not retreading the familiar. Systematically experimenting with what neural-like systems can generate gives us a new tool to investigate nature, culture, ideas, perception, and the workings of our own minds.\r\n\r\nOur interest in exploring the possibilities of machine intelligence in art could easily be justified on these grounds alone. But we feel that the stakes are much higher, for several reasons. One is that machine intelligence is such a profoundly transformational technology; it is about creating the very stuff of thought and mind. The questions of authenticity, reproducibility, legitimacy, purpose and identity that Walter Benjamin, Vilém Flusser, Donna Haraway and others have raised in the context of earlier technologies shift from metaphorical to literal; they become increasingly consequential. In the era where so many of us have become “information workers” (just as I am, in writing this piece), the issues raised by MI aren’t mere “theory” to be endlessly rehearsed by critics and journalists. We need to make decisions, personally and societally. A feedback loop needs to be closed at places like Google, where our work as engineers and researchers will have a real effect on how the technology is developed and deployed.\r\n\r\nThis requires that we apply ourselves rigorously and imaginatively across disciplines. The work can’t be done by technophobic humanists, any more than it can be done by inhuman technologists. Luckily, we are neither of the above. Both categories are stereotypes, if occasionally self-fulfilling ones, perpetuated by an unhelpful cultural narrative: the philistines again, claiming that artists are elves, and technical people dwarves, when of course the reality is that we are all (at least) human. There’s no shortage today of artists and intellectuals who, like Alberti, Holbein or Hockney, are eager to work with and influence the development of new technologies. There’s also no shortage of engineers and scientists who are thoughtful and eager to engage with artists and other humanists. And of course the binary is false; there are people who are simultaneously serious artists and scientists or engineers. We are lucky to have several such among our group of collaborators.\r\n\r\n[1] Braidotti, The Posthuman, p. 2.\r\n\r\n[2] Lawrence Weschler, seeing is forgetting the name of the thing one sees, p. 294.\r\n\r\n[3] The Hockney-Falco thesis is explained at length in the 2001 book Secret Knowledge: Rediscovering the Lost Techniques of the Old Masters. While critiques of their methodology and expository approach have been made, both by scientists and by art historians (for example, this one in 2004 from Christopher Tyler of the Smith-Kettlewell Eye Research Institute in San Francisco), the basic point, that the Old Masters used what was at the time state-of-the-art optical technology to render effects in painting, is not in serious dispute.\r\n\r\n[4] Link to the article, Portrait of the artist as a cheat.\r\n\r\n[5] Even works that eschew modern technology are often enriched owing to that choice — whether through new perspectives on traditional techniques, as with the analog techno ensemble Dawn of Midi, or through an aesthetic or even ethic of renunciation, as with Amish furniture made entirely by hand. These artistic or design choices are of course not “wrong”; on the contrary, their “rightness” exists in relation to the technology of the culture in which they are embedded, and they would be diminished without that context. We can be as awed by contemporary renunciatory art as the artists of the past would be by today’s “normal”.\r\n\r\n[6] From an evolutionary point of view it’s clear that other primate brains are closely analogous to those of humans, hence the widespread use of macaques for electrophysiological experiments. Many of Haraway’s contributions are, however, focused on behavioral and sociological studies, domains where she shows how the cultural priors of the research community inform which questions are asked, which observations are made, and which conclusions are drawn. There is an element of subjectivity and observer bias in every branch of science, but it’s especially pronounced in research areas that rely heavily on narrative and statistical observations.\r\n\r\n[7] Kember and Zylinska, Life after New Media: Mediation as a Vital Process, p. 193.\r\n\r\n[8] Turkle, Alone Together.\r\n\r\n[9] “Cyborg” is short for cybernetic organism, meaning a hybrid of machine and biology.\r\n\r\n[10] Flusser, Photo: towards a philosophy of photography, p. 23.\r\n[11] Flusser, Photo, p. 30–31.\r\n\r\n[12] As in Tosini et al., The Retina and Circadian Rhythms, Springer 2014, p. 3: “A remarkable piece of tissue, the retina is a true outpost of the brain, peripheral only for its location on the back of the eye.”\r\n\r\n[13] Similar aesthetic judgments (and impressive engineering feats to support them) were in play by the end of the film emulsion era. Kodacolor II had “as many as 12 emulsion layers, with upwards of 20 different chemicals in each layer”. This chemical programming embodied aesthetic judgments, just like the software that eventually replaced it. Aesthetics imply normativity, and therefore aren’t neutral with respect to subject matter; so for example, photo processing explicitly favored white people until late in the film era. Some digital camera software still reflects racial bias.\r\n\r\n[14] Jess Brillhart’s piece also makes use of virtual reality which, while not neural, represents an important advance in both cameras and displays.'), ('5', 'Musk Tries To Walk Back His Switch To Republican', 'Musk yesterday: “To be clear, I support the left half of the Republican Party and the right half of the Democratic Party!”\r\n\r\nEr…he delivered that statement at an elite REPUBLICAN gathering, and no, it wasn’t a gathering of the left half. It was full-on nutters. He was the guest of Kevin McCarthy, for instance. He’s a guy who recently announced that if Republicans win this fall they will intimidate those in Congress who continue to investigate Trump, January 6 or any of that. Really, Elon? You support January 6?\r\n\r\nI’m Confused\r\nMusk’s original announcement that he was switching to the Republican side came as rather a stunner to all the trendy hipsters who thought Elon like them was all about stopping climate change and renewable energy and all that.\r\n\r\nWell, anyone with clear vision (me!) could see all along that Elon was all about Elon and very little else.\r\n\r\nI’ve dedicated half my career as a satirist to skewering him because he’s such an easy target and so easy to hate.\r\n\r\nHere you will find an assortment of my anti-Elonism. It really is like a political stance for me, but it’s not Elon I really hate.\r\n\r\nFive Books That Everyone Thinks Elon Musk Should Read\r\nAnd a few articles, too.\r\nmedium.com\r\n\r\nIt’s our worship of him — and the other billionaires — that makes me sick.\r\n\r\nAnd like good worshippers of any cult, there is nothing Elon can do or say that will ever convince many an Elon-stan to wake up and smell the billionaire cofffee.\r\n\r\nIt stinks, that’s what it does.\r\n\r\nAnybody who claims to care about the environment cannot be supportive of the right half of the Democratic party.\r\n\r\nDuh! Elon, it was those right-wingers like the West Virginian asshole that just neutered the climate bill and passed a tax credit for electric cars that NO CARS NOT ONE ON THE MARKET qualify for.\r\n\r\nI repeat, thanks to our stalemate-stuck partisan method of governing, they make compromises like this, and the result is absurdities like a tax credit for NO CAR THAT CURRENTLY EXISTS — including Teslas, btw.\r\n\r\nElon said also that if the Republicans need to stay out of people’s bedrooms and the democrats need to stay out of people’s wallets.\r\n\r\nSocialism bad.\r\n\r\nWell, if I was an Elon-billionaire I don’t think I would be so god damned paranoid about my shekels. I would have more money than all my family and all my children and all their children and all their grandchildren could ever spend ever! So why the paranoia, Elon?\r\n\r\nRemember that Clinton slogan — it’s the economy stupid? We democrats aren’t about ruining the economy or stealing people’s money — that’s just right-wing propaganda. Since Clinton the democrats have actually been pretty well the bitch of Wall Street, didn’t you know that?\r\n\r\nWhere’s the socialist hellscape?\r\n\r\nIt’s in your imagination, dumbshit genius.\r\n\r\nDon’t get me started on Texas\r\nIf you wanted to support the left wing of the Republican party you moved to the wrong state. Maine maybe mighta been a better bet.\r\n\r\nThem Texans stand about as much a chance of staying out of people’s bedrooms as you stand of not having about nine more kids, super nerd. They already want to put people in jail who perform or have abortions. Soon being gay will be against the law again in a bunch of states like Texas.\r\n\r\nNo, you’re not going to tame that impala, my friend. The Republican big cats have tasted blood with this new Supreme Court and they’re just going to want more and more and more until we won’t even have bedrooms any more. We’ll live in Handmaiden-type dorms under a totalitarian theocracy.\r\n\r\nSurely you can see that, Elon?\r\n\r\nI mean, today it was announced that they were pulling a book about Anne Frank from the library shelves in Texas. Anne Frank, Elon. You really want to be on that team?\r\n\r\nNewsflash — Tesla is not cool any more\r\nEveryone I know in LA wants to dump their fucking Tesla and everyone I know hates Elon’s guts right now. So if you want to walk back your political blunder, I think you need to work a little harder, Republiconman.\r\n\r\nIs it all about that horse?\r\nThe day of Elon’s original announcement that he was switching parties came — coincidentally — on the same day news broke about him offering to buy a horse to a female employee if she gave him a handjob.\r\n\r\n“Democrat dirty tricks,” Elon claimed that day, as if the Dem party had an interest in Elon’s equestrian antics. Why would they?\r\n\r\nHe was trying to distract us from the horse, and maybe it worked. He’s pretty much like a smarter Donald Trump. “Fake news,” he called the charges. “Fake news,” he calls anything he doesn’t like.\r\n\r\nHere’s some fake news, a la about ten years ago:\r\n\r\nNewsflash: there’s this new kid, he’s like Thomas Edison, and he’s really cool, he smokes pot, and he’s built an electric car and a rocket ship and he’s going to take us to Mars and he’s just so fucking awesome.\r\n\r\nYeah, that turned out to be a bunch of hooey.\r\n\r\nElon is just another over-stuffed, overpaid, over-sexed, over-hyped bad actor out there playing his part in this pantomime we call Celebrity Billionaireism.\r\n\r\nI’m over it. Are you?');